This study deals with syntactic structure both in the broad sense(as opposed to semantics) and the narrow sense (as opposed tophonemics and morphology). It forms part of an attempt to constructa formalized general theory of linguistic structure and toexplore the foundations of such a theory. The search for rigorousformulation in linguistics has a much more serious motivation thanmere concern for logical niceties or the desire to purify well-establishedmethods of linguistic analysis. Precisely constructed modelsfor linguistic structure can play an important role, both negativeand positive, in the process of discovery itself. By pushing a precisebut inadequate formulation to an unacceptable conclusion, we canoften expose the exact source of this inadequacy and, consequently,gain a deeper understanding of the linguistic data. More positively,a formalized theory may automatically provide solutions for manyproblems other than those for which it was explicitly designed.Obscure and intuition-bound notions can neither lead to absurdconclusions nor provide new and correct ones, and hence they failto be useful in two important respects. I think that some of thoselinguists who have questioned the value of precise and technicaldevelopment of linguistic theory may have failed to recognize theproductive potential in the method of rigorously stating a proposedtheory and applying it strictly to linguistic material with no attemptto avoid unacceptable conclusions by ad hoc adjustments or looseformulation. The results reported below were obtained by aconscious attempt to follow this course systematically. Since thisfact may be obscured by the informality of the presentation, it isimportant to emphasize it here.
Specifically, we shall investigate three models for linguisticstructure and seek to determine their limitations. We shall find thata certain very simple communication theoretic model of languageand a more powerful model that incorporates a large part of whatis now generally known as "immediate constituent analysis" cannotproperly serve the purposes of grammatical description. The investigationand application of these models brings to light certainfacts about linguistic structure and exposes several gaps in linguistictheory; in particular, a failure to account for such relations betweensentences as the active-passive relation. We develop a third,transf ormational model for linguistic structure which is more powerfulthan the immediate constituent model in certain importantrespects and which does account for such relations in a natural way.When we formulate the theory of transformations carefully andapply it freely to English, we find that it provides a good deal ofinsight into a wide range of phenomena beyond those for which itwas specifically designed. In short, we find that formalization can,in fact, perform both the negative and the positive service commentedon above.
* INTRODUCTION
Syntax is the study of the principles and processes by which sentencesare constructed in particular languages. Syntactic investigationof a given language has as its goal the construction of a grammarthat can be viewed as a device of some sort for producing thesentences of the language under analysis. More generally, linguistsmust be concerned with the problem of determining the fundamentalunderlying properties of successful grammars. The ultimateoutcome of these investigations should be a theory of linguisticstructure in which the descriptive devices utilized in particulargrammars are presented and studied abstractly, with no specificreference to particular languages. One function of this theory is toprovide a general method for selecting a grammar for each language,given a corpus of sentences of this language.
The central notion in linguistic theory is that of "linguistic level."A linguistic level, such as phonemics, morphology, phrase structure,is essentially a set of descripti ve devices that are made available forthe construction of grammars; it constitutes a certain method forrepresenting utterances. We can determine the adequacy of alinguistic theory by developing rigorously and precisely the form ofgrammar corresponding to the set of levels contained within thistheory, and then investigating the possibility of constructing simpleand revealing grammars of this form for natural languages. Weshall study several different conceptions of linguistic structure inthis manner, considering a succession of linguistic levels of increasingcomplexity which correspond to more and more powerfulmodes of grammatical description; and we shall attempt to showthat linguistic theory must contain at least these levels if it is toprovide, in particular, a satisfactory grammar of English. Finally,we shall suggest that this purely formal investigation of the structureof language has certain interesting implications for semanticstudies.
1 The motivation for the particular orientation of the research reported hereis discussed below in \S 6.
* 2 THE INDEPENDENCE OF GRAMMAR
** 2.1 From now on I will consider a language to be a set (finite orinfinite) of sentences, each finite in length and constructed out of afinite set of elements. All natural languages in their spoken or writtenform are languages in this sense, since each natural language has afinite number of phonemes (or letters in its alphabet) and eachsentence is representable as a finite sequence of these phonemes (orletters), though there are infinitely many sentences. Similarly, theset of 'sentences' of some formalized system of mathematics can beconsidered a language. The fundamental aim in the linguisticanalysis of a language L is to separate the grammatical sequenceswhich are the sentences of L from the ungrammatical sequenceswhich are not sentences of L and to study the structure of thegrammatical sequences. The grammar of L will thus be a devicethat generates all of the grammatical sequences of L and none of theungrammatical ones. One way to test the adequacy of a grammarproposed for L is to determine whether or not the sequences that itgenerates are actually grammatical, i.e., acceptable to a nativespeaker, etc. We can take certain steps towards providing a behavioralcriterion for grammaticalness so that this test of adequacy canbe carried out. For the purposes of this discussion, however,suppose that we assume intuitive knowledge of the grammaticalsentences of English and ask what sort of grammar will be able todo the job of producing these in some effective and illuminatingway. We thus face a familiar task of explication of some intuitiveconcept - in this case, the concept "grammatical in English," andmore generally, the concept "grammatical. "
Notice that in order to set the aims of grammar significantly it issufficient to assume a partial knowledge of sentences and non-sentences. That is, we may assume for this discussion that certainsequences of phonemes are definitely sentences, and that certainother sequences are definitely non-sentences. In many intermediatecases we shall be prepared to let the grammar itself decide, when thegrammar is set up in the simplest way so that it includes the clearsentences and excludes the clear non-sentences. This is a famili arfeature of explication.\cite{l} A certain number of clear cases, then, willprovide us with a criterion of adequacy for any particular grammar.For a single language, taken in isolation, this provides only a weaktest of adequacy, since many different grammars may handle theclear cases properly. This can be generalized to a very strong condition,ho wever, if we insist that the clear cases be handled properlyfor each language by grammars all of which are constructed by thesame method. That is, each grammar is related to the corpus ofsentences in the language it describes in a way fixed in advance forall grammars by a given linguistic theory. We then have a verystrong test of adequacy for a linguistic theory that attemps to give ageneral explanation for the notion "grammatical sentence" i n termsof "observed sentence," and for the set of grammars constructed i naccordance with such a theory. I t is furthermore a reasonablerequirement, since we are interested not only in particular languages,but also in the general nature of Language. There is a great dealmore that can be said about this crucial topic, but this would takeus too far afield. Cf. \S 6.
\cite{1} Cf. , for example, N. Goodman, The structure of appearance (Cambridge,1 95 1 ), pp. 5-6. Notice that to meet the aims of grammar, given a linguistictheory, it is sufficient to have a partial knowledge of the sentences (i.e., acorpus) of the language, since a linguistic theory will state the relationbetween the set of observed sentences and the set of grammatical sentences ;i.e., it will define "grammatical sentence" in terms of "observed sentence,"certain properties of the observed sentences, and certain properties of grammars.To use Quine's formulation, a linguistic theory will give a general explanationfor what 'could' be in language on the basis of "what is plus simplicity of thelaws whereby we describe and extrapolate what is". (W. V. Quine, From alogical point of view (Cambridge, 1 953], p. 54). Cf. \S 6. 1 .~
** 2.2 On what basis do we actually go about separating grammaticalsequences from ungrammatical sequences ? I shall not attempt togive a complete answer t o this question here (cf. \S\S 6.7), but I wouldlike to point out that several answers that i mmediately suggestthemselves could not be correct. First, it is obvious that the set ofgrammatical sentences cannot be identi fied with any particularcorpus of utterances obtained by the linguist in his field work. Anygrammar of a language will pro ject the finite and somewhat accidentalcorpus of observed utterances to a set (presumably infinite)of grammatical utterances. In this respect, a grammar mirrors thebehavior of the speaker who, on the basis of a finite and accidentalexperience with language, can produce or understand an i ndefinitenumber of new sentences. Indeed, any explication of the notion" grammatical in L" (i.e., any characterization of "grammatical i nL " i n terms o f " observed utterance o f L") can b e thought o f a s offeringan explanation for this fundamental aspect of linguistic behavior.
** 2.3 Second, the notion "grammatical" cannot be identified with"meaningful" or "significant" in any semantic sense. Sentences (1)and (2) are equally nonsensical, but any speaker of English willrecognize that only the former i s grammatical.
(1) Colorless green ideas sleep furiously.
(2) Furiously sleep ideas green colorless.
Similarly, there is no semantic reason to prefer (3) to (5) or (4)to (6), but only (3) and (4) arc grammatical sentences of English.
(3) have you a book on modern music?
(4) the book seems interesting.
(5) read you a book on modern music?
(6) the child seems sleeping.
Such examples suggest that any search for a semantically baseddefinition of "grammaticalness" will be futile. We shall see, in fact,in \S 7, that there are deep structural reasons for distinguishing (3)and (4) from (5) and (6) ; but before we are able to find an explanationfor such facts as these we shall have to carry the theory ofsyntactic structure a good deal beyond its familiar limits.
** 2.4 Third, the notion "grammatical in English" cannot be identi-fied in any way with the notion "high order of statistical approximationto English." It is fair to assume that neither sentence (1) nor(2) (nor indeed any part of these sentences) has ever occurred in anEnglish discourse. Hence, in ,my statistical model for grammaticalness,these sentences will be ruled out on identical grounds asequally 'remote' from English. Yet (1), though nonsensical, isgrammatical, while (2) is not. Presented with these sentences, aspeaker of English will read (1) with a normal sentence intonation,but he will read (2) with a falling i ntonation on each word ; in fact,with just the intonation pattern given to any sequence of unrelatedwords. He treats each word in (2) as a separate phrase. Similarly,he will be able to recall (1) much more easily than (2), to learn itmuch more quickly, etc. Yet he may never have heard or seen anypair of words from these sentences joined in actual discourse. Tochoose another example, in the context "I saw a fragile-," thewords "whale" and " of" may have equal (i.e., zero) frequency in thepast linguistic experience of a speaker who will immediately recognize that one of these su bstitutions, but not the other, gives a grammaticalsentence. We cannot, of course, appeal to the fact that sentencessuch as (1) 'might' be uttered in some sufficiently far-fetchedcontext, while(2) would never be, since the basis for this differentiationbetween (1) and (2) is precisely what we are interested in determining.
Evidently, one's ability to produce and recognize grammaticalutterances is not based on notions of statistical approximation andthe like. The custom of calling grammatical sentences those that"can occur", or those that are "possi ble", has been responsible forsome confusion here. It is natural to understand "possible" asmeaning "highly probable" and to assume that the linguist's sharpd istinction between grammatical and ungrammatical\cite{2} is motivatedby a feeling that si nce the 'reality' of language is too complex to bedescribed completely, he must content himself with a schematized-version replacing "zero probability, and all extremely low probabilities,by impossible, and all higher probabilities by possible."\cite{3} Wesee, however, that this idea is quite incorrect, and that a structuralanalysis cannot be understood as a schematic summary developedby sharpening the blurred edges in the full statistical picture. If werank the sequences of a given length in order of statistical approximationto English, we will find both grammatical and ungrammaticalsequences scattered throughout the list ; there appears to be noparticular relation between order of approximation and grammaticalness.Despite the undeniable interest and importance of semanticand statistical studies of language, they appear to have no directrelevance to the problem of determining or characterizing the set ofgrammatical utterances. I think that we are forced to conclude thatgrammar is autonomous and independent of meaning, and thatprobabilistic models give no particular insight into some of thebasic problems of syntactic structure.\cite{4}
\footnote{2} Below we shall suggest that this sharp distinction may be modified in favorof a notion of levels of grammaticalness. But this has no bearing on the pointat issue here. Thus (1) and (2) will be at different levels of grammaticalness evenif (1) is assigned a lower degree of grammaticalness than. say, (3) and (4) ; butthey will be at the same level of statistical remoteness from English. The same istrue of an indefinite number of similar pairs.
\footnote{3} C. F. Hockett, A manual of phonology (Baltimore, 1 955), p. 1 0.

\footnote{4} We return to the question of the relation between semantics and syntax in\S\S 8, 9, where we argue that this relation can only be studied after the syntacticstructure has been determined on independent grounds. I think that much thesame thing is true of the relation between syntactic and statistical studies oflanguage. G iven the grammar of a language, one can study the use of thelanguage statistically in various ways ; and the development of probabilisticmodels for the use of language (as distinct from the syntactic structure oflanguage) can be quite rewarding. Cf. B. Mandelbrot, "Structure formelle destextes et communication : deux etudes," Word 1 0. 1 -27 (1954) ; H. A. Simon,"On a class of skew distribution functions," Biometrika 42.425-40 (1955).
One might seek to develop a more elaborate relation between statistical andsyntactic structure than the simple order of approximation model we haverejected. I would certainly not care to argue that any such relation is unthinkable,but I know of no suggestion to this effect that does not have obvious flaws.Notice, in particular, that for any n, we can find a string whose first n words mayoccur as the beginning of a grammatical sentence S, and whose last n words mayoccur as the ending of some grammatical sentence S2, but where S, must bedistinct from S_2. For example, consider the sequences of the form "the manwho . . . are here," where . . . may be a verb phrase of arbitrary length. Noticealso that we can have new but perfectly grammatical sequences of word classes,e.g., a sequence of adjectives longer than any ever before produced in thecontext "I saw a - house." Various attempts to explain the grammaticalungrammaticaldistinction, as in the case of (1), (2), on the basis of frequency ofsentence type, order of approximation of word class sequences, etc., will runafoul of numerous facts like these.
* 3 AN ELEMENTARY LINGUISTIC THEORY
** 3.1 Assuming the set of grammatical sentences of English to begiven, we now ask what sort of device can produce this set (equivalently,what sort of theory gives an adequate account of thestructure of this set of utterances). We can think of each sentenceof this set as a sequence of phonemes of finite length. A language isan enormously involved system, and it is quite obvious that anyattempt to present directly the set of grammatical phoneme sequenceswould lead to a grammar so complex that it would be practicallyuseless. For this reason (among others), lingu istic descriptionproceeds in terms of a system of "levels of representations."Instead of stating the phonemic structure of sentences directly, thelinguist sets up such 'higher level' elements as morphemes, andstates separately the morphemic structure of sentences and thephonemic structure of morphemes. It can easily be seen that thejoint description of these two levels will be much simpler than adirect description of the phonemic structure of sentences.
Let us now consider various ways of describing the morphemicstructure of sentences. We ask what sort of grammar is necessary togenerate all the sequences of morphemes (or words) that constitutegrammatical English sentences, and only these.
One requirement that a grammar must certainly meet is that it befinite. Hence the grammar cannot simply be a list of all morpheme(or word) sequences, since there are infinitely many of these. Afamiliar communication theoretic model for language suggests away out of this difficulty. Suppose that we have a machine that canbe in any one of a finite number of different internal states, andsuppose that this machine switches from one state to another byproducing a certain symbol (let us say, an English word). One ofthese states is an initial state ; another is a final state. Suppose thatthe machine begins in the i nitial state, runs through a sequence ofstates (producing a word with each transition), and ends in the finalstate. Then we call the sequence of words that has been produced a"sentence". Each such machine thus defines a certain language ;namely, the set of sentences that can be produced i n this way. Anylanguage that can be produced by a machine of this sort we call afinite state language ; and we can call the machine itself a finite stategrammar. A finite state grammar can be represented graphically i nthe form of a "state diagram".\cite{l} For example, the grammar thatproduces just the two sentences "the man comes" and "the mencome" can be represented by the following state diagram :
(7)
We can extend this grammar to produce an infinite number of sentencesby adding closed loops. Thus the finite grammar of thesubpart of English containing the above sentences in addition to"the old man comes", "the old old man comes", . . . , "the old mencome", "the old old men come", . . . , can be represented by thefollowing state diagram :
(8)
Given a state diagram, we produce a sentence by tracing a path fromthe initial point on the left to the final point on the right, alwaysproceeding i n the direction of the arrows. Having reached a certainpoint in the diagram, we can proceed along any path leading fromthis point, whether or not this path has been traversed before inconstructing the sentence in question. Each node in such a diagramthus corresponds to a state of the machine. We can allow transitionfrom one state to another in several ways, and we can have anyn umber of closed loops of any length. The machines that producelanguages in this manner are known mathematically as "finite stateMarkov processes." To complete this elementary communicationtheoretic model for language, we assign a probability to eachtransition from state to state. We can then calculate the "uncertainty"associated with each state and we can define the "informationcontent" of the language as the average uncertainty, weighted bythe probability of being in the associated states. Since we arestudying grammatical, not statistical structure of language here, thisgeneralization does not concern us.
In view of the generality of this conception of language, and itsutility in such related disciplines as communication theory, it isimportant to inquire i nto the consequences of adopting this point ofview in the syntactic study of some language such as English or aformalized system of mathematics. Any attempt to construct afinite state grammar for E nglish runs into serious difficulties andcomplications at the very outset, as the reader can easily convincehimself. However, it is unnecessary to attempt to show this by
This conception oflanguage is an extremely powerful and generalone. If we can adopt it, we can view the speaker as being essentiallya machine of the type considered. In producing a sentence, thespeaker begins in the initial state, produces the first word of thesentence, thereby switching into a second state which limits thechoice of the second word, etc. Each state through which he passesrepresents the grammatical restrictions that limit the choice of thenext word at this point in the utterance.\cite{2}
\footnote{2} This is essentially the model of language that Hockett develops in A manualof phonology (Baltimore, 1955), 02.
In view of the generality of this conception of language, and itsutility in such related disciplines as communication theory, it isimportant to inquire i nto the consequences of adopting this point ofview in the syntactic study of some language such as English or aformalized system of mathematics. Any attempt to construct afinite state grammar for E nglish runs into serious difficulties andcomplications at the very outset, as the reader can easily convincehimself. However, it is unnecessary to attempt to show this byexample, in view of the foIIowingmore general remark about English :
(9) English is not a finite state language.
That is, it is impossible, not just difficult, to construct a device of thetype described above (a diagram such as (7) or (8» which willproduce all and only the grammatical sentences of English . Todemonstrate (9) it is necessary to define the syntactic properties ofEnglish more precisely. We shall proceed to describe certainsyntactic properties of English which indicate that, under anyreasonable delimitation of the set of sentences of the language,(9) can be regarded as a theorem concerning English. To go back tothe question asked in the second paragraph of \S 3, (9) asserts that itis not possible to state the morphemic structure of sentencesdirectly by means of some such device as a state diagram, and thatthe Markov process conception of language outlined above cannotbe accepted, at least for the purposes of grammar.
** 3.2 A language is defined by giving its 'alphabet' (i.e., the finite setof symbols out of which its sentences are constructed) and itsgrammatical sentences. Before investigating English directly, let usconsider several languages whose alphabets contai n just the lettersa, b, and whose sentences are as defined in (10 i-iii) :
(10) (i) ab, aabb, aaabbb, . . . , and in general, all sentences consistingof n occurrences of a followed by n occurrences ofb and only these ;(ii) aa, bb, abba, baab, aaaa, bbbb, aabbaa, abbbba, . . " andin general , all sentences consisting of a string X followedby the 'mirror image' of X (i.e., X in reverse), and onlythese ;(iii) aa, bb, abab, baba, aaaa, bbbb, aabaab, abbabb, . . . , and i ngeneral, all sentences consisting of a string X of a's and b'sfollowed by the identical string X, and only these.
We can easily show that each of these three languages is not a finitestate language. Similarly, languages such as (10) where the a's andb's i n question are not consecutive, but are embedded in otherstrings, will fail to be finite state languages under quite generalconditions.\cite{3}
\footnote{3} See my "Three models for the description of language," I.R.E. Transaclionson Informalion Theory, vol. IT-2, Proceedings of the symposium on informationtheory, Sept., 1956, for a statement of such conditions and a proof of (9).Notice in particular that the set of well-formed formulas of any formalizedsystem of mathematics or logic will fail to wnstitute a finite state language,because of paired parentheses or equivalent restrictions.

But it is clear that there are subparts of English with the basicform of (10i) and (10ii). Let $S_1,S_2 ,S_3, \ldots$ be declarative sentencesin English. Then we can have such English sentences as :
(11) (i)If $S_1$, then $S_2$.(ii) Either $S_3$, or $S_4$.(iii) The man who said that $S_5$, is arriving today.
In (11i), we cannot have "or" in place of "then" ; in (11ii), wecannot have "then" in place of "or" ; in (11 iii), we cannot have"are" instead of "is". In each of these cases there is a dependencybetween words on opposite sides of the comma (i.e., "if"-"then","either"-"or", "man"-"is"). But between the interdependentwords, in each case, we can insert a declarative sentence $S_1$ , $S_3$, $S_5$,and this declarative sentence may in fact be one of (11i-iii). Thus ifin (11i) we take $S_1$ as (11ii) and $S_3$ as (11iii), we will have thesentence :
(12) if, either (11iii), or $S_4$, then $S_2$,
and $S_5$ in (11iii) may again be one of the sentences of (11). It is clear,then, that in English we can find a sequence $a + S_1 + b$, where thereis a dependency between a and b, and we can select as $S_1$ anothersequence containing $c + S_2 + d$, where there is a dependency betweenc and d, then select as $S_2$ another sequence of this form, etc. A setof sentences that is constructed in this way (and we see from (11)that there are several possibilities available for such construction(11)comes nowhere near exhausting these possibilities) will have allof the mirror image properties of(10ii) which exclude (10ii) from theset of finite state languages. Thus we can find various kinds of non-finite state models within English. This is a rough indication of thelines along which a rigorous proof of (9) can be given, on theassumption that such sentences as (11) and (12) belong to English,while sentences that contradict the cited dependencies of (11) (e.g.,"either $S_1$ , then $S_2$," etc.) do not belong to English. Note that manyof the sentences of the form (12), etc., will be quite strange andunusual (they can often be made less strange by replacing "if" by"whenever", "on the assumption that", "if it is the case that", etc.,without changing the substance of our remarks). But they are allgrammatical sentences, formed by processes of sentence constructionso simple and elementary that even the most rudimentaryEnglish grammar would contain them. They can be understood,and we can even state quite simply the conditions under which theycan be true. It is difficult to conceive of any possible motivation forexcluding them from the set of grammatical English sentences.Hence it seems quite clear that no theory of linguistic structure basedexclusively on Markov process models and the like, will be able toexplain or account for the ability of a speaker of English to produceand understand new utterances, while he rejects other new sequencesas not belonging to the language.
** 3.3 We might arbitrarily decree that such processes of sentenceformation in English as those we are discussing cannot be carriedout more than $n$ times, for some fixed $n$. This would of course makeEnglish a finite state language, as, for example, would a limitationof English sentences to length of less than a million words. Sucharbitrary limitations serve no useful purpose, however. The pointis that there are processes of sentence formation that finite stategrammars are intrinsically not equipped to handle. If these processeshave no finite limit, we can prove the literal inapplicability ofthis elementary theory. If the processes have a limit, then theconstruction of a finite state grammar will not be literally out of thequestion, since it will be possible to list the sentences, and a list isessentially a trivial finite state grammar. But this grammar will beso complex that it will be of little use or interest. In general, theassumption that languages are infinite is made in order to simplifythe description of these languages. If a grammar does not haverecursive devices (closed loops, as in (8), in the finite state grammar)it will be prohibitively complex. If it does have recursive devices ofsome sort, it will produce infinitely many sentences.
In short, the approach to the analysis of grammaticalness suggestedhere in terms of a finite state Markov process that producessentences from left to right, appears to lead to a dead end just assurely as the proposals rejected in \S 2. If a grammar of this typeproduces all English sentences, it will produce many non-sentencesas well. If it produces only English sentences, we can be sure thatthere will be an infinite number of true sentences, false sentences,reasonable questions, etc., which it simply will not produce.
The conception of grammar which has just been rejected representsin a way the minimal linguistic theory that merits seriousconsideration. A finite state grammar is the simplest type ofgrammar which, with a finite amount of apparatus, can generate aninfinite number of sentences. We have seen that such a li mitedlinguistic theory is not adequate ; we are forced to search for somemore powerful type of grammar and some more 'abstract' form oflinguistic theory. The notion of "linguistic level of representation"put forth at the outset of this section must be modified and elaborated.At least one linguistic level cannot have this simple structure.That is, on some level, it will not be the case that each sentence isrepresented simply as a finite sequence of elements of some sort,generated from left to right by some simple device. Alternatively,we must give up the hope of finding a finite set of levels, orderedfrom high to low, so constructed that we can generate all utterancesby stating the permitted sequences of highest level elements, theconstituency of each highest level element in terms of elements ofthe second level, etc., finally stating the phonemic constituency ofelements of the next-to-lowest level.\cite{4} At the outset of \S 3, weproposed that levels be established in this way in order to simplif ythe description of the set of grammatical phoneme sequences. If alanguage can be described in an elementary, left-to-right manner interms of a single level (i.e., if it is a finite state language) then thisdescription may i ndeed be simplified by construction of such higherlevels ; but to generate non-finite state languages such as English weneed fundamentally different methods, and a more general conceptof "linguistic level".
\footnote{4} A third alternative would be to retain the notion of a linguistic level as asimple linear method of representation, but to generate at least one such levelfrom left to right by a device with more capacity than a finite state Markovprocess. There are so many difficulties with the notion of linguistic level basedon left to right generation, both in terms of complexity of description and lackof explanatory power (cf. \S 8), that it seems pointless to pursue this approachany further. The grammars that we discuss below that do not generate fromleft to right also correspond to processes less elementary than finite state Markovprocesses. But they are perhaps less powerful than the kind of device thatwould be required for direct left-to-right generation of English. Cf. my "Threemodels for the description of language" for some futher discussion.

* 4 PHRASE STRUCTURE** 4.1Customarily, linguistic description on the syntactic level isformulated i n terms o f constituent analysis (parsing). W e now askwhat form of grammar is presupposed by description of this sort.We find that the new form of grammar is essentially more powerfulthan the finite state model rejected above, and that the associatedconcept of "linguistic level" is different in fundamental respects.
As a simple example of the new form for grammars associatedwith constituent analysis, consider the fol lowing :(13) (i) $Sentence -> NP + VP$
(ii) $NP -> T + N$
(iii) $VP -> Verb + NP$
(iv) $T -> the$
(v) $N -> man, ball, etc.$
(vi) $Verb -> hit, took, etc.$
Suppose that we interpret each rule $X -> Y$ of (13) as the instruction"rewrite X as Y". We shall call (14) a derivation of the sentence"the man hit the ball." where the numbers at the right of each lineof the derivation refer to the rule of the "grammar" (13) used inconstructing that line from the preceding line.\cite{l}
(14) $Sentence$
$NP + VP$  (i)
$T + N + VP$ (ii)
$T + N + Verb + NP$  (iii)
$the + N + Verb + NP$  (iv)
$the + man + Verb + NP$  (v)
$the + man + hit + NP$  (vi)
$the + man + hit + T + N$  (ii)
$the + man + hit + the + N$  (iv)
$the + man + hit + the + ball$  (v)
Thus the second line of (14) is formed from the first line by rewriting$Sentence$ as $NP + VP$ in accordance with rule (i) of (13) ; the thirdline is formed from the second by rewriting $NP$ as $T + N$ in accordancewith rule (ii) of (13) ; etc. We can represent the derivation (14)in an obvious way by means of the following diagram:
(15) tree structure
The diagram (15) conveys less information than the derivation (14),since it does not tell us in what order the rules were applied in (14).Given (14), we can construct (15) uniquely, but not vice versa, sinceit is possible to construct a derivation that reduces to (15) with adifferent order of application of the rules. The diagram (15) retainsjust what is essential in (14) for the determination of the phrasestructure (constituent analysis) of the derived sentence "the manhit the ball." A sequence of words of this sentence is a constituentof type Z if we can trace this sequence back to a single point oforigin in (15), and this point of origin is labelled Z. Thus "hit theball" can be traced back to $VP$ in (15) ; hence "hit the ball" is a VPin the derived sentence. But "man hit" cannot be traced back toany single point of origin in (J 5); hence "man hit" is not a constituentat all.
\footnote{1}The numbered rules of English grammar to which ref erence will constantlybe made in the following pages are collected and properly ordered in \S 12,Appendix II. The notational conventions that we shall use throughout thediscussion of English structure are stated in \S I , Appendix I.In his "Axiomatic syntax : the construction and evaluation of a syntacticcalculus," Language 3 1 .409-1 4 (1955), Harwood describes a system of wordclass analysis similar in form to the system developed below for phrase structure.The system he describes would be concerned only with the relation betweenT + N + Verb + T + N and the + man + hit + the + ball in the example discussedi n (13)-(15) ; i.e., the grammar would contain the "initial string" T + N + Verb +T + N and such rules as (13iv-vi). It would thus be a weaker system than theelementary theory discussed in \S 3, since it could not generate an infinite languagewith a finite grammar. While Harwood's formal account (pp. 409-11)deals only with word class analysis, the linguistic application (p. 412) is a case ofimmediate constituent analysis, with the classes $C_{i.. m}$ presumably taken to beclasses of word sequences. This extended application is not quite compatiblewith the formal account, however. For example, none of the proposed measuresof goodness of fit can stand without revision under this reinterpretation of theformalism.

We say that two derivations are equivalent if they reduce to thesame diagram of the form (15). Occasionally, a grammar maypermit us to construct nonequivalent derivations for a given sentence.Under these circumstances, we say that we have a case of"constructional homonymity",\cite{2} and if our grammar is correct, thissentence of the language should be ambiguous. We return to theimportant notion of constructional homonymity below.
\footnote{2}See \S8 .1 for some examples of constructional homonymity. See my Thelogical structure of linguistic theory (mimeographed) ; "Three models for thedescription of language" (above, p. 22, fn. 3) ; C. F. Hockett, "Two models ofgrammatical description," Linguistics Today, Word 10.210--33 (1954) ; R. S.Wells, "Immediate constituents," Language 23.81 - 117 ( 1947) for more detaileddiscussion.
One generalization of (13) is clearly necessary. We must be ableto limit application of a rule to a certain context. Thus T can berewritten a if the following noun is singular, but not if it is plural ;similarly, Verb can be rewritten "hits" if the preceding noun is man,but not if it is men. In general, if we wish to limit the rewriting ofX as Y to the context Z - W, we can state in the grammar the rule
(16) Z + X + W -> Z + Y + W.
For example, in the case of singular and plural verbs, instead ofhaving Verb -> hits as an additional rule of (13), we should have
(17) $NP_{sing}$ + Verb -> $NP_{sing}$ + hits
indicating that Verb is rewritten hits only in the context $NP_{sing-}$.Correspondingly, (13ii) will have to be restated to include NPsingand $NP_{pl}$.\cite{3} This is a straightforward generalization of (13). Onefeature of (13) must be preserved , however, as it is in (17) : only asingle element can be rewritten i n any single rule ; i.e., in (16), Xmust be a single symbol such as T, Verb, and not a sequence suchas T + N. If this condition is not met, we will not be able to recoverproperly the phrase structure of derived sentences from theassociated diagrams of the form (15), as we did above.
\footnote{3}Thus in a more complete grammar, (13 ii) might be replaced by a set ofrules that includes the following:NP ->iNP" no}LNP.,N p,'n. -> T + N + fl) ( + Prepositional Phrase)NP., ->T+ N + S ( + Prepositional Phrase)where S is the morpheme which is singular for verbs and plural for nouns("comes," "boys"), and fl) is the morpheme which is singular for nouns andplural for verbs ("boy," "come"). We shall omit all mention of first and secondperson throughout this discussion. Identification of the nominal and verbalnumber affix is actually of questionable validity.
We can now describe more generally the form of grammarassociated with the theory of linguistic structure based uponconstituent analysis. Each such grammar is defined by a finite set Eof initial strings and a finite set F of 'instruction formulas' of theform X -> Y in .erpreted : "rewrite X as Y. " Though X need not bea single symbol, only a single symbol of X can be rewritten informing Y. In the grammar (13), the only member of the set E ofinitial strings was the single symbol Sentence, and F consisted of therules (i) - (vi) ; but we might want to extend E to include, forexample, Declarative Sentence, Interrogative Sentence, as additionalsymbols. Given the grammar [E, FJ, we define a derivation as afinite sequence of strings, beginning with an initial string of E, andwith each string in the sequence being derived from the precedingstring by application of one of the instruction form ulas of F. Thus( 14) is a derivation, and the five-termed seq lIence of strings consistingof the first five lines of (14) is also a deriv�tion. Certainderivations are terminated derivations, in the sense that their finalstring cannot be rewritten any further by the rules F. Thus (14) is aterminated derivation, but the sequence consisting of the first fivelines of (14) is not. If a string is the last line of a terminated derivation,we say that it is a terminal string. Thus the + man + hit + the +ball is a terminal string from the grammar (13). Some grammars ofthe form [L, F) may have no terminal strings, but we are interestedonly in grammars that do have terminal strings, i .e., that describesome language. A set of strings is called a terminal language if it isthe set of terminal strings for some grammar [L, F). Thus each suchgrammar defines some terminal language (perhaps the 'empty'language containing no sentences), and each terminal language isproduced by some grammar of the form [L, F). Given a terminallanguage and its grammar, we can reconstruct the phrase structureof each sentence of the language (each terminal string of thegrammar) by considering the associated diagrams of the form (15),as we saw above. We can also define the grammatical relations inthese languages in a formal way in terms of the associated diagrams.
** 4.2 In \S 3 we considered languages, calJed "finitt: state languages",which were generated by finite state M arkov processes. Now weare considering terminal languages that are generated by systems ofthe form $[\Sigma, F]$. These two types of languages are related in thefollowing way
Theorem: Every finite state language is a terminal language, butthere are terminal languages which are not finite state languages.\cite{4}
The import of this theorem is that description in terms of phrasestructure is essentially more powerful than description in terms ofthe elementary theory presented above in \S 3. As examples ofterminal languages that are not finite state languages we have thelanguages (10i), (10ii) discussed in \S 3. Thus the language (10i),consisting of all and only the strings ab, aabb, aaabbb, . . . can beproduced by the $[\Sigma, F]$ grammar (18).
(18) L: ZF : Z -> abZ -> aZb
Th is gram mar has the initial string Z (as (13) has the initial stringSentence) and it has two rules. It can easily be seen that eachterminated derivation constructed from (18) ends in a string of thelanguage (lOi), and that all such strings are produced in this way.Similarly, languages of the form (10ii) can be produced by $[\Sigma, F]$grammars (lOiii), however, cannot be produced by a grammar ofthis type, unless the rules embody contextual restrictions.\cite{5}
\footnote{4} See my " Three models for the description of language" (above, p. 22, fn. 3)for proofs of this and related theorems about relative power of grammars.
In \S 3 we pointed out that the languages (1O i) and (1O ii) correspondto subparts of English, and that therefore the finite stateMarkov process model is not adequate for English. We now seethat the phrase structure model does not fail in such cases. We havenot proved the adequacy of the phrase structure model, but we haveshown that large parts of English which literally cannot be describedin terms of the finite-state process model can be descri bed in termsof phrase structure.
Note that in the case of (18), we can say that in the string aaabbbof (10i), for example, ab is a Z, aabb is !l Z, and aaabbb itself is a Z. \cite{6}Th us this particular string contains three 'phrases,' each of whichis a Z. This is, of course, a very trivial language. It is important toobserve that in describing this language we have introduced asymbol Z which is not contained in the sentences of this language.This is the essential fact about phrase structure which gives it its'abstract' character.

\footnote{5}See my "On certain formal properties of grammars", Inf ormation andControl 2. 1 3 3-167 (1959).
\footnote{6}Where "is a" is the relation defined in \S 4.1 in terms of such diagrams as (15).
Observe also that in the case of both (13) and (18) (as in everysystem of phrase structure), each terminal string has many differentrepresentations. For example, in the case of (13), the terminalstring "the man hit the ball" is represented by the strings Sentence,NP + V P, T + N + VP, and all the other lines of (14), as well as bysuch strings as NP + Verb + NP, T + N + hit + NP, which wouldoccur in other derivations equivalent to (14) in the sense theredefined. On the level of phrase structure, then, each sentence of thelanguage is represented by a set of strings, not by a single string as itis on the level of phonemes, morphemes, or words. Thus phrasestructure, taken as a linguistic level, has the fundamentally differentand nontrivial character which, as we saw in the last paragraph of\S 3, is required for some linguistic level. We cannot set up a hierarchyamong the various representations of "the man hit the ball" ;we cannot subdivide the system of phrase structure into a finite setof levels, ordered from higher to lower, with one representation foreach sentence on each of these sublevels. For example, there is noway of ordering the elements N P and V P relative to one another.Noun phrases are contained within verb phrases, and verb phraseswithin noun phrases, in English. Phrase structure mllst be consideredas a single level, with a set of representations for eachsentence of the language. There is a one-one correspondencebetween the properly chosen sets of representations, and diagramsof the form (15).
** 4.3 Suppose that by a [I, F] grammar we can generate all of thegrammatical sequences of morphemes of a language. In order tocomplete the grammar we must state the phonemic structure ofthese morphemes, so that the grammar will prod uce the grammaticalphoneme sequences of the language. But this statement (which wewould call the morpiJophonemics of the language) can also be givenby a set of rules of the form "rewrite X as Y", e.g. , for English,
(19) (i) walk � /w;)k/(ii) take + past � /tuk/(iii) hit + past � /hit/(iv) /. . . D/ + past � / . . . D/ + jJd/ (where 0 = /t/ or /d/)(v) / " , Cunv! + past � /. " Cunv/ + /t/ (where Cuny is an unvoicedconsonant)(vi) past � /d/.(vii) take � /teyk/etc.
or something similar. Note, incidentally, that order must be definedamong these rules - e.g., (ii) must precede (v) or (vii), or we willderive such forms as /teykt/ for the past tense of take. In thesemorphophonemic rules we need n o longer require that only a singlesymbol be rewritten in each rule.
We can now extend the phras ; structure derivations by applying(19), so that we have a unified process for generating phonemesequence from the ini tial string Sentence. This makes it appear asthough the break between the higher level of phrase structure andthe lower levels is arbitrary. Actually, the distinction is not arbitrary.For one thing, as we have seen, the formal properties of therules X � Y corresponding to phrase structure are different fromthose of the morphophonemic rules, since in the case of the formerwe must require that only a single symbol be rewritten. Second, theelements that figure in the rules (19) can be classified into a finite setof levels (e.g., phonemes and morphemes ; or, perhaps, phonemes,morphophonemes, and morphemes) each of which is elementary inthe sense that a single string of elements of this level is associatedwith each sentence as its representation on this level (except incases of homonymity), and each such string represents a singlesentence. But the elements that appear in the rules correspondingto phrase structure cannot be classified into higher and lower levelsin this way. We shall see below that there is an even more fundamentalreason for marking this subdivison i nto the higher levelrules of phrase structure and the lower level rules that convertstrings of morphemes into strings of phonemes.
The formal properties of the system of phrase structure make aninteresting study, and it is easy to show that further elaboration ofthe form of grammar is both necessary and possible. Thus it caneasily be seen that it would be quite advantageous to order the rulesof the set F so that certain of the rules can apply only after othershave applied . For example, we should certainly want all rules of theform (17) to apply before any rule which enables us to rewrite NPas NP + Preposition + NP, or the like ; otherwise the grammar willproduce such non sentences as "the men near the truck begins workat eight." But this elaboration leads to problems that would carryus beyond the scope of this study.
* Limitations of phrase structure description** 5.1 We have discussed two models for the structure of language,a communication theoretic model based on a conception of languageas a Markov process and corresponding, in a sense, to the minimallinguistic theory, and a phrase structure model based on immediateconstituent analysis. We have seen that the first is surely inadequatefor the purposes of grammar, and that the second is more powerfulthan the first, and does not fail in the same way. Of course thereare languages (in our general sense) that cannot be described interms of phrase structure, but I do not k now whether or notEnglish is itself literal ly outside the range of such analysis. However,I think that there are other grounds for rejecting the theory of phrasestructure as inadequate for the purpose of linguistic description.
The strongest possible proof of the inadequacy of a linguistictheory is to show that it literally cannot apply to some naturallanguage. A weaker, but perfectly sufficient demonstration of inadequacywould be to show that the theory can apply only clumsily ;that is, to show that a ny grammar that can be constructed in termsof this theory will be extremely complex, ad hoc, and 'unrevealing',that certain very simple ways of describing grammatical sentencescannot be accommodated withjn the associated forms of grammar,and that certain fundamental formal properties of natural languagecannot be utilized to simplify grammars. We can gather a good dealof evidence of this sort in favor of the thesis that the form of grammardescribed above, and the conception of linguistic theory thatunderlies it, are fundamentally inadequate.
The only way to test the adequacy of our present apparatus is toattempt to apply it d irectly to the description of English sentences.As soon as we consider any sentences beyond the simplest type, andin particular, when we attempt to define some order among therules that produce these sentences, we find that we run into numerousdifficulties and complications. To give substance to this claimwould require a large expenditure of effort and space, and I canonly assert here that this can be shown fairly convincingly.\cite{1}Instead of undertaking this rather arduous and ambitious coursehere, I shall limit myself to sketching a few simple cases in whichconsiderable improvement is possible over grammars of the form$[\Sigma, F]$. In \S 8 I shall suggest an independent method of demonstratingthe inadequacy of constituent analysis as a means of describingEnglish sentence structure.
\footnote{1}See my The logical structure of linguistic theory for detailed analysis of thisproblem.
5.2 One of the most productive processes for forming new sentencesis the process of conjunction. If we have two sentencesZ + X + W and Z + Y + W, and if X and Y are actually constituentsof these sentences, we can generally form a new sentence Z - X +and + Y - W. For example, from the sentences (20a-b) we can formthe new sentence (21).
(20) (a) the scene - of the movie - was in Chicago(b) the scene - of the play - was in Chicago
(21) the scene - of the movie and of the play - was in Chicago.
If X and Y are, however, n ot constituents, we generally cannot dothis.\cite{2} For example we cannot form (23) from (22a-b).
(22) (a) the - liner sailed down the - river(b) the - tugboat chugged up the - river
(23) the - liner sailed down the and tugboat chugged up the - river.
Similarly, if X and Y are both constituents, but are constituents ofdifferent kinds (i.e., if in the diagram of the form (15) they each havea single origin, but this origin is labelled differently), then we cannotin general form a new sentence by conjunction. For example, wecannot form (25) from (24a-b).
(24) (a) the scene - of the movie - was in Chicago(b) the scene - that I wrote - was in Chicago
(25) the scene - of the movie and that I wrote - was in Chicago
In fact, the possibility of conjunction offers one of the best criteriafor the initial determination of phrase structure. We can simplifythe description of conjunction if we try to set up constituents insuch a way that the following rule will hold :
(26) If S 1 and S2 are grammatical sentences, and S I differs from S2only in that X appears in S 1 where Y appears in S2 (i.e.,S 1 =. . X . . and S2 = .. Y . . ), and X and Y are constituents ofthe same type in Sl and S2' respectively, then S3 is a sentence,where S3 is the result of replacing X by X + and + Y in SI(i.e., S3 = . . X + and + Y . . ).
Even though additional qualification is necessary here, the grammaris enormously simplified if we set up constituents in such a way that(26) holds even approximately. That is, it is easier to state thedistribution of "and" by means of qualifications on this rule thanto do so directly without such a rule. But we now face the followingdifficulty : we cannot incorporate the rule (26) or anything like it ira grammar [I, F] of phrase structure, because of certain fundamentallimitations on such grammars. The essential property of rule(26) is that in order to apply it to sentences Sl and S2 to form thenew sentence S3 we must k now not only the actual form of Sland S2 but also their constituent structure - we must know not onlythe final shape of these sentences, but also their 'history of derivation.'But each rule X -> Y of the grammar [I, F) applies or failsto apply to a given string by virtue of the actual substance of thisstring. The question of how this string gradually assumed this formis irrelevant. If the string contains X as a substring, the rule X -> Ycan apply to it ; if not, the rule cannot apply.

\footnote{2}(21) and (23) are extreme cases in which there is no question about thepossibility of conjunction. There are many less clear cases. For example, it isobvious that "John enjoyed the book and liked the play" (a string of the formNP - VP + and + VP) is a perfectly good sentence, but many would question thegrammaticalness of, e.g., "John enjoyed and my friend liked the play" (a stringof the form NP + Verb + and + Verb - NP). The latter sentence, in whichconjunction crosses over constituent boundaries, is much less natural than thealternative "John enjoyed the play and my friend liked it", but there is nopref erable alternative to the former. Such sentences with con junction crossingconstituent boundaries are also, in general, marked by special phonemic featuressuch as extra long pauses (in our example, between "liked" and "the"), contrastivestress and intonation, failure to reduce vowels and drop final consonants inrapid speech, etc. Such features normally mark the reading of non-grammaticalstrings. The most reasonable way to describe this situation would seem to be bya description of the following kind : to form fully grammatical sentences byconjunction, it is necessary to conjoin single constituents ; if we con join pairs ofconstituents, and these are major constituents (i.e., 'high up' in the diagram(15)), the resulting sentences are semi-grammatical ; the more completely weviolate constituent structure by conjunction, the less grammatical is the resultingsentence. This description requires that we generalize the grammaticalungrammaticaldichotomy, developing a notion of degree of grammaticalness.It is immaterial to our discussion, however, whether we decide to exclude suchsentences as "John enjoyed and my friend liked the play" as ungrammatical,whether we include them as semi-grammatical, or whether we include them asfully grammatical but with special phonemic features. In any event they forma class of utterances distinct from "John en joyed the play and liked the book,"etc., where constituent structure is preserved perfectly ; and our conclusion thatthe rule for conjunction must make explicit reference to constituent structuretherefore stands, since this distinction will have to be pointed out in the grammar.

We can put this somewhat differently. The grammar $[\Sigma, F]$ canalso be regarded as a very elementary process that generatessentences not from "left to right" but from "top to bottom".Suppose that we have the following grammar of phrase structure :(27) $\Sigma$ : SentenceF : $X_1$ -> $Y_1$$\dots$$X_n$ -> $Y_n$ .
Then we can represent this grammar as a machine with a finitenumber of internal states, including an initial and a final state. Inits initial state it can produce only the element Sentence, therebymoving into a new state. It can then produce any string Yj such thatSentence -> Yj is one of the rules of F in (27), again moving into anew state. Suppose that Yj is the string . . . Xj . . . Then the machinecan produce the string . . . Y j . . . by "applying" the rule Xj -> }j.The machine proceeds in this way from state to state until it final lyproduces a terminal string ; it is now in the final state. The machinethus produces derivations, in the sense of \S4. The important pointis that the state of the machine is completely determined by thestring it has j ust produced (i.e., by the last step of the derivation) ;more specifically, the state is determined by the subset of 'left-hand'elements X, of F which are contained in this last-produced string.But rule (26) requires a more powerful machine, which can "lookback" to earlier strings in the derivation in order to determine howto produce the next step in the derivation.
Rule (26) is also fundamentally new in a different sense. It makesessential reference to two distinct sentences Sl and S2' but ingrammars of the [L, F] type, there is no way to incorporate suchdouble reference. The fact that rule (26) cannot be incorporatedinto the grammar of phrase structure indicates that even if thisform for grammar is not literally inapplicable to English, it is certainlyinadequate in the weaker but sufficient sense consideredabove. This rule leads to a considerable simplification of thegrammar ; in fact, it provides one of the best criteria for determininghow to set up constituents. We shall see that there are many otherrules of the same general type as (26) which play the same dual role.
** 5.3 In the grammar (13) we gave only one way of analyzing theelement Verb, namely, as hit (cf. (13vi)). But even with the verbalroot fixed (let us say, as take), there are many other forms that thiselement can assume, e.g., takes, has + taken, will + take, has + been +taken, is + being + taken, etc. The study of these "auxiliary verbs"turns out to be quite crucial in the development of English grammar.We shall see that their behavior is very regular and simply describablewhen observed from a point of view that is quite different fromthat developed above. though it appears to be quite complex if weattempt to incorporate these phrases directly into a $[\Sigma, F]$ grammar.
Consider first the auxiliaries that appear unstressed ; for example,"has" in "John has read the book" but not "does" in "John doesread books."\cite{3} We can state the occurrence of these auxiliaries indeclarative sentences by adding to the grammar (13) the followingrules :
(28) (i)Verb � Aux + V(ii)V � hit, take, walk, read, etc.(iii)Aux � C(M) (have + en) (be + ing) (be + en)(iv)M � will, can, may, shall, must
(29) (i)Is in the context N PSing- j C � 0 in the context N P pL- (past \cite{4}(ii)Let AI stand for any of the affixes past, S, 0 , en, ing. Letv stand for any M or V, or have or be (i.e., for any nonaffixin the phrase Verb). Then :AI+ v � v + AI '* ,where '* i s interpreted as word boundary.\cite{5}(iii) Replace + by '* except in the context v - Af Insert '*initially and finally.
The interpretation of the notations in (28 iii) is as follows : we mustchoose the element C, and we may choose zero or more of theparenthesized elements in the given order. In (29 i) we may developC into any of three morphemes, observing the contextual restrictionsgiven. As an example of the application of these rules, we constructa derivation in the style of (14), omitting the initial steps.
(30) the + man + Verb + the + book from (13 i-v)the + man + Aux + V + the + book (28 i)the + man + A ux + read + the + book (28 ii)the + man + C + have + en + be + ing + read + the + book(28 iii) - we select theelements C, hal'e + enand be + ing.the + man + S + have + en + be + ing + read + the + book(29 i)the + man + have + S * be + en * read + ing * the + book(29 ii) - three times.# the # man # have + S # be + en # read+ ing # the # book #(29 iii)
The morphophonemic rules (19), etc., will convert the last line ofthis derivation into :
(31) the man has been read ing the book
in phonemic transcription. Similarly, every other auxiliary verbphrase can be generated. We return later to the question of furtherrestrictions that must be placed on these rules so that only grammaticalsequences can be generated . Note, incidentally, that themorphophonemic rules will have to include such ru les as : lI'ill + S ->will, will + past -> II'0U/d. These rules can be dropped if we rewrite(28 iii) so that either C or M, but n ot both , can be selected . But nowthe forms would, could, miglit, sliould must be added to (28 iv), andcertain 'sequence of tense' statements become more complex. It isimmateral to our further discllssion which of these alternativeanalysesis iadopted , Several other minor revisions are possible,

\footnote{3} We return to the stressed auxiliary "do" below, in \S 7.1 (45)-(47) .
\footnote{4} We assume here that (13 ii) has been extended in the manner of fn. 3,above, p. 29, or something similar.\footnote{5} If we were formulating the theory of grammar more carefully, we wouldinterpret # as the concatenation operator on the level of words, while + is theconcatention operator on the level of phrase structure. (29) would then be partof the definition of a mapping which carries certain objects on the level of phrasestructure (essentially, diagrams of the form (15)) into strings of words. See myThe logical structure of linguistic theory for a more careful formulation.
Notice that in order to apply (29i) in (30) we had to lise the factthat the + man is a singular noun phrase N Psilly' That is, we had torefer back to some earlier step in the derivation in order to determinethe constituent structure of tlie + man. (The alternative of ordering(29i) and the rule that develops N PSillY into the + man in such a waythat (29i) must precede the latter is not possible, for a variety ofreasons, some of which appear below). H ence, (29i), just like (26),goes beyond the elementary Markovian character of grammars ofphrase structure, and cannot be incorporated within the $[\Sigma, F]$grammar.

Rule (29ii) violates the requirements of $[\Sigma, F]$ grammars evenmore severely, It also requires reference to constituent structure(i.e., past history of derivation) and in addition, we have no way toexpress the required inversion within the terms of phrase structure.Note that this rule is useful elsewhere in the grammar, at least inthe case where Afis ing. Thus the morphemes to and ing play a verysimilar role within the noun phrase in that they convert verb phrasesinto noun phrases, giving, e.g.,
(32) . th t th {to prove that theorem } was difficult.provmg a eorem
etc. We can exploit this parallel by adding to the grammar (13) therule
(33) NP --> e:g} vp
The rule (29 ii) will then convert ing + prove + that + theorem intoproving * that + theorem. A more detailed analysis of the VP showsthat this parallel extends much further than this, in fact.
The reader can easily determine that to duplicate the effect of(28 iii) and (29) without going beyond the bounds of a system [L, F]of phrase structure, it would be necessary to give a fairly complexstatement. Once again, as in the case of conjunction, we see thatsignificant simplification of the grammar is possible if we arepermitted to formulate rules of a more complex type than those thatcorrespond to a system of immediate constituent analysis. Byallowing ourselves the freedom of (29 ii) we h ave been able to statethe constituency of the auxiliary phrase in (28 iii) without regard tothe interdependence of its elements, and it is always easier to describea sequence of independent elements than a sequence of mutuallydependent ones. To put the same thing differently, in the auxiliaryverb phrase we really have discontinuous elements - e.g. , in (30),the elements have . . en and be . . ing. But discontinuities cannot behandled within $[\Sigma, F]$ grammars.\cite{6} In (28iii) we treated theseelements as continuous, and we introduced the discontinuity by thevery simple additional rule (29 ii). We shall see below, in \S 7, thatthis analysis of the element Verb serves as the basis for a far-reachingand extremely simple analysis of several important features ofEnglish syntax.
\footnote{6}We might attempt to extend the notions of phrase structure to accountfor discontinuities. It has been pointed out several times that fairly seriousdifficulties arise in any systematic attempt to pursue this course. Cf. my"System of syntactic analysis," Journal of Symbolic Logic 1 8.242-56 (1953) ;C. F. Hockett, "A formal statement of morphemic analysis," Studies in Linguistics10.27-39 (1952) ; idem, "Two models of grammatical description,"Linguistics Today, Word 1 0.2 1 0-33 (1954). Similarly, one might seek to remedysome of the other deficiencies of [E, FJ grammars by a more complex accountof phrase structure. I think that such an approach is ill-advised, and that it canonly lead to the development of ad hoc and fruitless elaborations. It appears tobe the case that the notions of phrase structure are quite adequate for a smallpart of the language and that the rest of the language can be derived by repeatedapplication of a rather simple set of transformations to the strings given by thephrase structure grammar. If we were to attempt to extend phrase structuregrammar to cover the entire language directly, we would lose the simplicity ofthe limited phrase structure grammar and of the transformational development.This approach would miss the main point of level construction (cf. first paragraphof \S 3 . 1 ), namely, to rebuild the vast complexity of the actual languagemore elegantly and systematically by extracting the contribution to this complexityof several linguistic levels, each of which is simple in itself.
** 5.4 As a third example of the inadequacy of the conceptions ofphrase structure, consider the case of the active-passive relation.Passive sentences are formed by selecting the element be + en in rule)8 iii). B ut there are heavy restrictions on this element that make itunique among the elements of the auxiliary phrase. For one thing,be + en can be selected only if the following V is transitive (e.g.,was + eaten is permitted, but not was + occurred) ; but with a fewexceptions the other elements of the auxiliary phrase can occurfreely with verbs. Furthermore, be + en cannot be selected if theverb V is followed by a noun phrase, as in (30) (e.g., we cannot ingeneral have NP + is + V + en + NP, even when V is transitive - wecannot have "lunch is eaten John"). Furthermore, if V is transitiveand is followed by the prepositional phrase by + NP, then we mustselect be + en (we can have "lunch is eaten by John" but not "Johnis eating by lunch," etc.). Finally, note that in elaborating (13) intoa full-fledged grammar we will have to place many restrictions onthe choice of Vin terms of subject and object in order to permit suchsentences as : "John admires sincerity," "sincerity frightens John,""John plays golf," "John drinks wine," while excluding the 'inverse'non-sentences7 "sincerity admires John," "John frightens sincerity,""golf p lays John," "wine drinks John". But this whole network ofrestrictions fails completely when we choose be + en as part of theauxiliary verb . In fact, in this case the same selectional dependencieshold, but in the opposite order. That is, for every sentenceN PI - V - N P 2 we can have a corresponding sentence N P 2 - is + Ven- by + NPI • If we try to include passives directly in the grammar(13), we shall have to restate all of these restrictions in the oppositeorder for the case in which be + en is chosen as part of the auxiliaryverb. This inelegant duplication, as well as the special restrictionsinvolving the element be + en, can be avoided only if we deliberatelyexclude passives from the grammar of phrase structure, andreintroduce them by a rule such as :
(34) If SI is a grammatical sentence of the formNPI - Aux - V - NP2,then the corresponding string of the formNP2 - Aux + be + en - V - by + NPIis also a grammatical sentence.
For example, if John - C - admire - sincerity is a sentence, thensincerity - C + be + en - admire - by + John (which by (29) and(19) becomes "sincerity is admired by John") is also a sentence.
\footnote{7}Here too we might make use of a notion of levels of grammaticalness assuggested in footnote 2, p. 35. Thus "sincerity admires John," though clearlyless grammatical than "John admires sincerity," is certainly more grammaticalthan "of admires John," I believe that a workable notion of degree of grammaticalnesscan be developed in purely formal terms (cf. my The logical structure oflinguistic theory), but this goes beyond the bounds of the present discussion.See \S 7.5 for an even stronger demonstration that inversion is necessary in thepassive.

We can now drop the element be + en, and all of the specialrestrictions associated with it, from (28 iii). The fact that be + enrequires a transitive verb, that it cannot occur before V + NP, that itmust occur before V + by + N P (where Vis transitive), that it invertsthe order of the surrounding noun phrases, is in each case anautomatic consequence of rule (34). This rule thus leads to a considerablesimplification of the grammar. But (34) is well beyond thelimits of $[\Sigma, F] grammars. Like (29 ii), it requires reference to theconstituent structure of the string to which it applies and it carriesout an inversion on this string in a structurally determined manner.
** 5.5 We have discussed three rules ((26), (29), (34)) which materiallysimplify the description of English but which cannot be incorporatedinto a $[\Sigma, F]$ grammar. There are a great many other rules of thistype, a few of which we shall discuss below. By further study of thelimitations of phrase structure grammars with respect to English wecan show quite conclusively that these grammars will be so hopelesslycomplex that they will be without interest unless we incorporatesuch rules.
If we examine carefully the implications of these supplementaryrules, however, we see that they lead to an entirely new conceptionof linguistic structure. Let us call each such rule a "grammaticaltransformation ." A grammatical transformation $T$ operates on agiven string (or, as in the case of (26), on a set of strings) with agiven constituent structure and converts it into a new string with anew derived constituent structure. To show exactly how thisoperation is performed requires a rather elaborate study whichwould go far beyond the scope of these remarks, but we can in factdevelop a certain fairly complex but reasonably natural algebra oftransformations having the properties that we apparently requirefor grammatical description.\cite{8}
\footnote{8} See my "Three models for the description of language" (above, p. 22, fn. 3)for a brief account of transformations, and {\it The logical structure of linguistictheory} and {\it Transformational Analysis} for a detailed development of transformationalalgebra and transformational grammars. See Z. S. Harris, "Cooccurrenceand Transformations in linguistic structure," Language 33.283-340(1957), for a somewhat different approach to transformational analysis.

From these few examples we can already detect some of theessential propelties of a transformational grammar. For one thing,it is clear that we must define an order of application on thesetransformations. The passive transformation (34), for example,must apply before (29). It must precede (29i), in particular, so thatthe verbal element in the resulting sentence will have the samenumber as the new grammatical subject of the passive sentence.And it must precede (29ii) so that the latter rule will apply properlyto the new inserted element be + en. (In discussing the question ofwhether or not (29i) can be fitted into a $[\Sigma, F]$ grammar, we mentionedthat this rule could not be required to appiy before the ruleanalyzing N P'ing into the + man, etc. One reason for this is nowobvious - (29i) must apply after (34), but (34) must apply after theanalysis of NPsing, or we will not have the proper selectionalrelations between the subject and verb and the verb and 'agent' inthe passive.)
Secondly, note that certain transformations are obligatory,whereas others are only optional. For example, (29) must be appliedto every derivation, or the result will simply not be a sentence.\cite{9}But (34), the passive transformation, may or may not be applied inany particular case. Either way the result is a sentence. Hence (29)is an obligatory transformation and (34) is an optional transformation.
\footnote{9}But of the three parts of (29i), only the third is obligatory. That is, pastmay occur after $NP_{sing}$. or $NP_{pl}. Whenever we have an element such as $C$ in (29i)which must be developed, but perhaps in several alternative ways, we can orderthe alternatives and make each one but the last optional, and the last, obligatory.
This distinction between obligatory and optional transformationsleads us to set up a fundamental distinction among the sentences ofthe language. Suppose that we have a grammar $G$ with a $[\Sigma, F]$ partand a transformational part, and suppose that the transformationalpart has certain obligatory transformations and certain optionalones. Then we define the kernel of the language (in terms of thegrammar G) as the set of sentences that are produced when weapply obligatory transformations to the terminal strings of the$[\Sigma, F]$ grammar. The transformational part of the grammar will beset up in such a way that transformations can apply to kernelsentences (more correctly, to the forms that underlie kernel sentences-i.e., to terminal strings of the $[\Sigma, F]$ part of the grammar)or to prior transforms. Thus every sentence of the language willeither belong to the kernel or will be derived from the stringsunderlying one or more kernel sentences by a sequence of one ormore transformations.
From these considerations we are led to a picture of grammars aspossessing a natural tripartite arrangement. Corresponding to thelevel of phrase structure, a grammar has a sequence of rules of theform X -> Y, and corresponding to lower levels it has a sequence ofmorphophonemic rules of the same basic form. Linking these twosequences, it has a sequence of transformational rules. Thus thegrammar will look something like this :
(35) $\Sigma$ : Sentence :F : XI ---> YI ] X. --> y.: Phrase structureTl ] Transformational structureTjZI ---> WI ] : MorphophonemicsZm ---> Wm
To produce a sentence from such a grammar we construct anextended derivation beginning with $Sentence$. Running throughthe rules of $F$ we construct a terminal string that will be a sequenceof morphemes, though not necessarily in the correct order. We thenrun through the sequence of transformations $T_1, \ldots T_j$, applyingeach obligatory one and perhaps certain optional ones. Thesetransformations may rearrange strings or may add or deletemorphemes. As a result they yield a string of words. We then runthrough the morphophonemic rules, thereby converting this stringof words into a string of phonemes. The phrase structure segmentof the grammar will include such rules as those of (13), (17) and (28).The transformational part will include such rules as (26), (29) and(34), formulated properly in the terms that must be developed in afull-scale theory of transformations. The morphophonemic partwill include such rules as (19). This sketch of the process of generationof sentences must (and easily can) be generalized to allow forproper functioning of such rules as (26) which operate on a set ofsentences, and to allow transformations to reapply to transforms sothat more and more complex sentences can be produced.
When we apply only obligatory transformations in the generationof a given sentence, we call the resulting sentence a kernel sentence.Further investigation would show that in the phrase structure andmorphophonemic parts of the grammar we can also extract askeleton of obligatory rules that must be applied whenever we reachthem in the process of generating a sentence. In the last few paragraphsof \S 4 we pointed out that the phrase structure rules lead to aconception of linguistic structure and "level of representation" thatis fundamentally different from that provided by the morphophonemicrules. On each of the lower levels corresponding to the lowerthird of the grammar an utterance is, in general, represented by asingle sequence of elements. But phrase structure cannot be brokendown into sublevels : on the level of phrase structure an utterance isrepresented by a set of strings that cannot be ordered into higher orlower levels. This set of representing strings is equivalent to adiagram of the form (15). On the transformational level, anutterance is represented even more abstractly in terms of a sequenceof transformations by which it is derived, ultimately from kernelsentences (more correctly, from the strings which underlie kernelsentences). There is a very natural general definition of "linguisticlevel" that includes all of these cases,\cite{10} and as we shall see later,there is good reason to consider each of these structures to be alinguistic level.

\footnote{10}Cf. The logical structure of linguistic theory and Transf ormational Analysis.

When transformational analysis is properly formulated we findthat it is essentially more powerful than description in terms ofphrase structure, just as the latter is essentially more powerfull thandescription in terms of finite state Markov processes that generatesentences from left to right. In particular, such languages as (lOiii)which lie beyond the bounds of phrase structure description withcontext-free rules can be derived transformationaIly.\cite{11} It isimportant to observe that the grammar is materially simplified whenwe add a transformational level, since it is now necessary to providephrase structure directly only for kernel sentences - the terminalstrings of the $[\Sigma, F]$ grammar are just those which underlie kernelsentences. We choose the kernel sentences in such a way that theterminal strings underlying the kernel are easily derived by meansof a $[\Sigma, F]$ description, while all other sentences can be derived fromthese terminal strings by simply statable transformations. We haveseen, and shall see again below, several examples of simplificationsresulting from transformational analysis. Full-scale syntacticinvestigation of English provides a great many more cases.
\footnote{11}Let G be a $[\Sigma, F]$ grammar with the initial string Sentence and with the setof all finite strings of a's and b's as its terminal output. There is such a grammar.Let G' be the grammar which contains G as its phrase structure part, supplementedby the transformation T that operates on any string $K$ which is a$Sentence$, converting it into $K + K$. Then the output of G' is (lOiii). Cf. p. 31.
One further point about grammars of the form (35) deservesmention, since it has apparently led to some misunderstanding.We have described these grammars as devices for generatingsentences. This formulation has occasionally led to the idea thatthere is a certain asymmetry in grammatical theory in the sensethat grammar is taking the point of view of the speaker ratherthan the hearer ; that it is concerned with the process of producingutterances rather than the 'inverse' process of analyzing andreconstructing the structure of given utterances. Actually, grammarsof the form that we have been discussing are quite neutralas between speaker and hearer, between synthesis and analysisof utterances. A grammar does not tell us how to synthesize aspecific utterance ; it doe·s not tell us how to analyze a particulargiven utterance. In fact, these two tasks which the speaker andhearer must perform are essentially the same, and are both outsidethe scope of grammars of the form (35). Each such grammar issimply a description of a certain set of utterances, namely, thosewhich it generates. From this grammar we can reconstruct theformal relations that hold among these utterances in terms of thenotions of phrase structure, transformational structure, etc. Perhapsthe issue can be clarified by an analogy to a part of chemical theoryconcerned with the structurally possible compounds. This theorymight be said to generate all physically possible compounds just asa grammar generates all grammatically 'possible' utterances. Itwould serve as a theoretical basis for techniques of qualitativeanalysis and synthesis of specific compounds, just as one might relyon a grammar in the investigation of such special problems asanalysis and synthesis of particular utterances.

* On the goals of liguistic theory
** 6.1In \S\S 3, 4 two models of linguistic structure were developed : asimple communication theoretic model and a formalized version ofimmediate constituent analysis. Each was found to be inadequate,and in \S 5 I suggested a more powerful model combining phrasestructure and grammatical transformations that might remedy theseinadequacies. Before going on to explore this possiblity, I wouldlike to clarify certain points of view that underlie the whole approachof his study.Our fundamental concern throughout this discussion of linguisticstructure is the problem of justification of grammars. A grammarof the language L is essentially a theory of L. Any scientific theoryis based on a finite number of observations, and it seeks to relate theobserved phenomena and to predict new phenomena by constructinggeneral laws in terms of hypothetical constructs such as (inphysics, for example) "mass" and "electron." Similarly, a grammarof English is based on a finite corpus of utterances (observations),and it will contain certain grammatical rules (laws) stated in termsof the particular phonemes, phrases, etc., of English (hypotheticalconstructs). These rules express structural relations among thesentences of the corpus and the indefinite number of sentencesgenerated by the grammar beyond the corpus (predictions). Ourproblem is to develop and clarify the criteria for selecting the correctgrammar for each language, that is, the correct theory of thislanguage.
Two types of criteria were mentioned in \S 2.1. Clearly, everygrammar will have to meet certain external conditions of adequacy;e.g., the sentences generated will have to be acceptable to the nativespeaker. In \S 8 we shall consider several other external conditions ofthis sort. In addition, we pose a condition of generality on grammars; we require that the grammar of a given language be constructedin accordance with a specific theory of linguistic structure inwhich such terms as "phoneme" and "phrase" are defined independentlyof any particular language.\cite{l} If we drop either the externalconditions or the generality requirement, there will be no way tochoose among a vast number of totally different 'grammars,' eachcompatible with a given corpus. But, as we observed in \S 2.1, theserequirements jointly give us a very strong test of adequacy for ageneral theory of linguistic structure and the set of grammars thatit provides for particular languages.
\footnote{1} I presume that these two conditions are similar to what Hjelmslev has inmind when he speaks of the appropriateness and arbitrariness of linguistic theory.Cf. L. Hjelmslev, Prolegomena to a theory of language = Memoir 7, IndianaUniversity Publications Antropology and Linguistics (Baltimore, 1953), p. 8.See also Hockett's discussion of "metacriteria" for linguistics ("Two models ofgrammatical description," Linguistics Today, Word 10.232-3) in this connection.
Notice that neither the general theory nor the particular grammarsare fixed for all time, in this view. Progress and revision maycome from the discovery of new facts about particular languages,or from purely theoretical insights about organization of linguisticdata - that is, new models for linguistic structure. But there is alsono circularity in this conception. At any given time we can attemptto formulate as precisely as possible both the general theory and theset of associated grammars that m ust meet the empirical, externalconditions of adequacy.
We have not yet considered the following very crucial question:What is the relation between the general theory and the particulargrammars that follow from it? In other words, what sense can wegive to the notion "follow from," in this context? It is at this pointthat our approach will diverge sharply from many theories oflinguistic structure.
The strongest requirement that could be placed on the relationbetween a theory of linguistic structure and particular grammars isthat the theory must provide a practical and mechanical method foractually constructing the grammar, given a corpus o f utterances.Let us say that such a theory provides us with a discovery procedurefor grammars.
A weaker requirement would be that the theory must provide apractical and mechanical method for determining whether or not agrammar proposed for a given corpus is, in fact, the best grammarof the language from which this corpus is drawn. Such a theory,which is not concerned with the question of how this grammar wasconstructed, might be said to provide a decision procedure forgrammars.
An even weaker requirement would be that given a corpus andgiven two proposed grammars G1 and G2, the theory must tell uswhich is the better grammar of the language from which the corpusis drawn. In this case we might say that the theory provides anevaluation procedure for grammars.
These theories can be represented graphically in the followingmanner.(36) 
(i) CORPUS
(li)
(iii)
Figure (36i) represents a theory conceived as a machine with acorpus as its input and a grammar as its output; hence, a theory thatprovides a discovery procedure. (36ii) is a device with a grammarand a corpus as its inputs, and the answers "yes" or "no" as itsoutputs, as the grammar is or is not the correct one; hence, itrepresents a theory that provides a decision procedure for grammars.(36 iii) represents a theory with grammars G$_1$ and G$_2$ and acorpus as inputs, and the more preferable of G$_1$ and G$_2$ as output ;hence a theory that provides an evaluation procedure for grammars.\cite{2}
\footnote{2} The basic question at issue is not changed if we are willing to accept asmall set of correct grammars instead of a single one.
The point of view adopted here is that it is unreasonable todemand of linguistic theory that it provide anything more than apractical evaluation procedure for grammars. That is, we adopt theweakest of the three positions described above. As I interpret mostof the more careful proposals for the development of linguistictheory,\cite{3} they attempt to meet the strongest of these three requirements.That is, they attempt to state methods of analysis that aninvestigator might actually use, if he had the time, to construct agrammar of a language directly from the raw data. I think that it isvery questionable that this goal is attainable in any interesting way,and I suspect that any attempt to meet it will lead into a maze ofmore and more elaborate and complex analytic procedures thatwill fail to provide answers for many important questions about thenature of linguistic structure. I believe that by lowering our sightsto the more modest goal of developing an evaluation procedure forgrammars we can focus attention more clearly on really crucialproblems of linguistic structure and we can arrive at more satisfyinganswers to them. The correctness of this judgment can only bedetermined by the actual development and comparison of theoriesof these various sorts. Notice, however, that the weakest of thesethree requirements is still strong enough to guarentee significancefor a theory that meets it. There are few areas of science in which onewould seriously consider the possibility of developing a general,practical, mechanical method for choosing among several theories,each compatible with the available data.
\footnote{3} For example, B. Bloch, "A set of postulates for phonemic analysis,"Language 24.3-46 (1948) ; N. Chomsky, "Systems of syntactic analysis,"Journal of Symbolic Lagic 1 8.242-56 (1953) ; Z. S. Harris, "From phoneme tomorpheme," Language 31. 190-222(1955); idem, Methods in structural linguistics(Chicago, 1951); C. F. Hockett, "A formal statement of morphemic analysis,"Studies in Linguistics 10.27-39 (1952); idem, "Problems of morphemic analysis,"Language 23.321-43 (1947); R. S. Wells, "Immediate constituents," Language23.81-117 (1947); and many other works. Although discovery procedures arethe explicit goal of these works, we often find on careful examination that thetheory that has actually been constructed furnishes no more than an evaluationprocedure for grammars. For example, Hockett states his aim in "A formalstatement of morphemic analysis" as the development of "formal proceduresby which one can work from scratch to a complete description of the pattern ofa language" (p.27); but what he actually does is describe some of the formalproperties of a morphological analysis and then propose a "criterion wherebythe relative efficiency of two possible morphic solutions can be determined;with that, we can choose the maximally efficient possibility, or, arbitrarily, anyone of those which are equally efficient but more efficient than all others" (p.29).
In the case of each of these conceptions of linguistic theory wehave qualified the characterization of the type of procedure by theword "practical". This vague qualification is crucial for an empiricalscience. Suppose, for example, that we were to evaluate grammarsby measuring some such simple property as length. Then itwould be correct to say that we have a practical evaluation procedurefor grammars, since we could count the number of symbolsthey contain; and it would also be literally correct to say that wehave a discovery procedure, since we can order all sequences of thefinite number of symbols from which grammars are constructed interms of length, and we can test each of these sequences to see if itis a grammar, being sure that after some finite amount of time weshall find the shortest sequence that qualifies. But this is not thetype of discovery procedure that is contemplated by those who areattempting to meet the strong requirement discussed above.
Suppose that we use the word "simplicity" to refer to the set offormal properties of grammars that we shall consider in choosingamong them. Then there are three main tasks in the kind of programfor linguistic theory that we have suggested. First, it isnecessary to state precisely (if possible, with operational, behavioraltests) the external criteria of adequacy for grammars. Second, wemust characterize the form of grammars in a general and explicitway so that we can actually propose grammars of this form forparticular languages. Third, we must analyze and define the notionof simplicity that we intend to use in choosing among grammars allof which are of the proper form. Completion of the latter two taskswill enable us to formulate a general theory of linguistic structure inwhich such notions as "phoneme in L", "phrase in L", "transformationin L" are defined for an arbitrary language L in terms ofphysical and distributional properties of utterances of L and formalproperties of grammars of L.\cite{4} For example, we shall define the setof phonemes of L as a set of elements which have certain physicaland distributional properties, and which appear in the simplestgrammar for L. Given such a theory, we can attempt to constructgrammars for actual languages, and we can determine whether ornot the simplest grammars that we can find (i.e., the grammars thatthe general theory compels us to choose) meet the external conditionsof adequacy. We shall continue to revise our notions ofsimplicity and out characterization of the form of grammars untilthe grammars selected by the theory do meet the external conditions. \cite{5}Notice that this theory may not tell us, in any practical way, howto actually go about constructing the grammar of a given languagefrom a corpus. But it must tell us how to evaluate such a grammar;it must thus enable us to choose between two proposed grammars.
\footnote{4}Linguistic theory will thus be formulated in a metalanguage to the languagein which grammars are written - a metametalanguage to any languagefor which a grammar is constructed.
\footnote{5}We may in fact revise the criteria of adequacy, too, in the course of research.That is, we may decide that certain of these tests do not apply to grammaticalphenomena. The subject matter of a theory is not completely determinedin advance of investigation. It is partially determined by the possibility ofgiving an organized and systematic account of some range of phenomena.

In the preceding sections of this study we have been concernedwith the second of these three tasks. We have assumed that the setof grammatical sentences of English is given and that we have somenotion of simplicity, and we have tried to determine what sort ofgrammar will generate exactly the grammatical sentences in somesimple way. To formulate this goal in somewhat different terms,we remarked above that one of the notions that m ust be defined ingeneral linguistic t heory is "sentence in L." Entering into thedefinitions will be such terms as "observed utterance in L", "simplicityof the grammar of L," etc. This general theory is accordinglyconcerned with clarifying the relation between the set of grammaticalsentence and the set of observed sentences. Our investigation ofthe structure of the former set is a preparatory study, proceedingfrom the assumption that before we can characterize this relationclearly, we will have to know a great deal more about the formalproperties of each of these sets.
In \S 7 below, we shall continue to investigate the relative complexityof various ways of uescribing English structure. In particular,we shall be concerned with the question of whether the wholegrammar is simplified if we consider a certain class of sentences tobe kernel sentences or if we consider them to be derived by transformation.We thus arrive at certain decisions about the structureof English. In \S 8 we shall argue that there is independent evidencein favor of our method for selecting grammars. That is, we shall tryto show that the simpler grammars meet certain external conditionsof adequacy while the more complex grammars that embodydifferent decisions about assignment of sentences to the kernel, etc.,fail these conditions. These results can be no more than suggestive,however, until we give a rigorous account of the notion of simplicityemployed. I think that such an account can be given, but this wouldgo beyond the scope of the present monograph. Nevertheless, itshould be fairly clear that under any reasonable definition of"simplicity of grammar", most of the decisions about relativecomplexity that we reach below will stand.\cite{6}
\footnote{6}See my {\it The logical Structure of linguistic theory} for discussion of methodsfor evaluating grammars in terms of formal properties of simplicity.We are not, incidentally, denying the usefullness of even partially adequatediscovery procedures. They may provide valuable hints to the practicing linguistor they may lead to a small set of grammars that can then be evaluated. Ourmain point is that a linguistic theory should not be identified with a manualof useful procedures, nor should it be expected to provide mechanical proceduresfor the discovery of grammars.
Notice that simplicity is a systematic measure; the only ultimatecriterion in evaluation is the simplicity of the whole system. Indiscussing particular cases, we can only indicate how one or anotherdecision will affect the over-all complexity. Such validation canonly be tentative, since by simplifying one part of the grammar wemay complicate other parts. It is when we find that simplificationof one part of the grammar leads to corresponding simplification ofother parts that we feel that we are really on the right track. Below,we shall try to show that the simplest transformational analysis ofone class of sentences does quite frequently clear the way to asimpler analysis of other classes.
In short, we shall never consider the question of how one mighthave arrived at the grammar whose simplicity is being determined;e.g., how one might have discovered the analysis of the verb phrasepresented in \S 5.3. Question of this sort are not relevant to theprogram of research that we have outlined above. One may arriveat a grammar by intuition, guess-work, all sorts of partial methodologicalhints, reliance on past experience, etc. It is no doubt possibleto give an organized account of many useful procedures of analysis,but it is questionable whether these can be formulated rigorously,exhaustively and simply enough to qualify as a practical and mechanicaldiscovery procedure. At any rate, this problem is not withinthe scope of our investigations here. Our ultimate aim is to providean objective, non-intuitive way to evaluate a grammar once presented,and to compare it with other proposed grammars. We are thusinterested in describing the form of grammars (equivalently, thenature of linguistic structure) and investigating the empirical consequencesof adopting a certain model for linguistic structure,rather than in showing how, in principe, one might have arrived atthe grammar of a language.
** 6.2 Once we have disclaimed any intention of finding a practicaldiscovery procedure for grammars, certain problems that have beenthe subject of intense methodological controversy simply do notarise. Consider the problem of interdependence of levels. It hasbeen correctly pointed out that if morphemes are defined in terms ofphonemes, and, simultaneously, morphological considerations areconsidered relevant to phonemic analysis, then linguistic theory maybe nullified by a real circularity. However, interdependence of levelsdoes not necessarily lead to circularity. In this case, for example, wecan define "tentative phoneme set" and "tentative morpheme set"independently and we can develop a relation of compatibility thatholds between tentative phoneme sets and tentative morpheme sets.We can then define a pair of a phoneme set and a morpheme set fora given language as a compatible pair of a tentative phoneme set anda tentative morpheme set. Our compatibility relation may beframed partially in terms of simplicity considerations; that is, wemay define the phonemes and morphemes of a language as thetentative phonemes and morphemes which, among other things,jointly lead to the simplest grammar. This gives us a perfectlystraightforward way to define interdependent levels with no circularity.Of course, it does not tell us how to find the phonemes andmorphemes in a direct, mechanical way. But no other phonemic ormorphological theory really meets this strong requirement either,and there is little reason to believe that it can be met significantly.In any event, when we lower our aims to the development of anevaluation procedure, there remains little motivation for anyobjection to mixing of levels, and there is no difficulty in avoidingcircularity in the definition of interdependent levels.\cite{7}
\footnote{7}See Z. S. H arris, Methods in structural linguistics (Chicago, 1 9 5 1 ) (e.g.,Appendix to 7.4, Appendix to 8.2, chapters 9, 12) for examples of procedureswhich lead to interdependent levels. I think that Fowler's objections to Harris'morphological procedures (cL Language 28.504-9 [1952]) can be met withoutdifficulty by a noncircular formulation of the type just proposed. Cf. C. F.Hockett, A manual of phonology = Memoir 11, Indiana University PublicaTionsin Anthropology and Linguistics (Baltimore, 1955); idem, "Two fundamentalproblems in phonemics," Studies in Linguistics 7.33 (1949); R. Jakobson, "Thephonemic and grammatical aspects of language and their interrelation,"Proceedings of the Sixth International Congress of Linguists 5-1 8 (Paris, 1948);K. L . Pike, "Grammatical prerequisites to phonemic analysis," Word 3.1 55-72(1947); idem, "More on grammatical prerequisites," Word 8.106-2 1 (1952), forfurther discussion on interdependence of levels. Also N. Chomsky, M. Halle,F. Lukoff, "On accent and juncture in English," For Roman lakobson ('s-Gravenhage,1956), 65-80.
Bar-Hillel has suggested in "Logical syntax and semantics", Language30.230-7 (1954) that Pike's proposals can be formalized without the circularitythat many sense in them by the use of recursive definitions. He does not pursuethis suggestion in any detail, and my own feeling is that success along these linesis unlikely. Moreover, if we are satisfied with an evaluation procedure forgrammars, we can construct interdependent levels with only direct definitions, aswe have just seen.
The problem of interdependence of phonemic and morphemic levels must notbe confused with the question of whether morphological information is requiredto read a phonemic transcription. Even if morphological considerations areconsidered relevant to determining the phonemes of a language, it may still bethe case that the phonemic transcription provides complete 'reading' rules withno reference to other levels. Cf. N. Chomsky, M. Halle, F. Lukoff, "On accentand juncture in English," For Roman Jakobsoll ('s-Gravenhage, 1956), 65-80,for discussion and examples.

Many problems of morphemic analysis also receive quite simplesolutions if we adopt the general framework outlined above. Inattempting to develop discovery procedures for grammars we arenaturally led to consider morphemes as classes of sequences ofphonemes, i.e., as having actual phonemic 'content' in an almostliteral sense. This leads to trouble in such well-known cases asEnglish "took" /tuk/, where it is difficult without artificiality toassociate any part of this word with the past tense morpheme whichappears as /t/ in "walked" /w'Jkt/, as /d/ in "framed" /freymd/, etc.We can avoid all such problems by regarding morphology andphonology as two distinct but interdependent levels of representation,related in the grammar by morphophonemic rules such as(19). This "took" is represented on the morphological level as$take + past$ just as "walked" is represented as $walk + past$. Themorphophonemic rules (19ii), (19v), respectively, carry these stringsof morphemes into /tuk/, /w'Jkt/. The only difference between thetwo.cases is that (19v) is a much more general rule than (19ii).\cite{8} If wegive up the idea that higher levels are literally constructed out oflower level elements, as I think we must, then it becomes much morenatural to consider even such abstract systems of representation astransformational structure (where each utterance is represented bythe sequence of transformations by which it is derived from aterminal string of the phrase structure grammar) as consti tutinga linguistic level.
\footnote{8}Hockett gives a very clear presentation of this approach to levels inA manual of phonology (1955), p.15 . In "Two models of grammatical description,"Linguistics Today, Word 10.210-33 (1954), Hockett rejected a solutionvery much like the one we have just proposed on the grounds that " took and takeare partly similar in phonemic shape just as are baked and bake, and similar inmeaning also in the same way; this fact should not be obscured" (p.224). Butthe similarity in meaning is not obscured in our formulation, since the morphemepast appears in the morphemic representation of both "took" and"baked." And the similarity in phonemic shape can be brought out in the actualformulation of the morphophonemic rule that carries take + past into /tuk/.We will no doubt formulate this rules as
$ey -> u$ in the context $t - k + past$
in the actual morphophonemic statement. This will allow us to simplify thegrammar by a generalization that will bring out the parallel between "take""took,""shake"-"shook," "forsake"-"forsook," and more generally. "stand"-"stood,"etc.
We are not actually forced to give up hope of finding a practicaldiscovery procedure by adopting either the view that levels areinterdependent, or the conception of linguistic levels as abstractsystems of representation related only by general rules. Nevertheless,I think it is unquestionable that opposition to mixing levels,as well as the idea that each level is literally constructed out of lowerlevel elements, has its origin in the attempt to develop a discoveryprocedure for grammars. If we renounce this goal and if we distinguishclearly between a manual of suggestive and helpfulprocedures and a theory of linguistic structure, then there is littlereason for maintaining either of these rather dubious positions.
There are many other commonly held views that seem to losemuch of their appeal if we formulate our goals in the mannersuggested above. Thus it is sometimes argued that work on syntactictheory is premature at this time in view of the fact that manyof the problems that arise on the lower levels of phonemics andmorphology are unsolved. It is quite true that the higher levels oflinguistic description depend on results obtained at the lower levels.But there is also a good sense in which the converse is true. Forexample, we have seen above that it would be absurd, or evenhopeless, to state principles of sentence construction in terms ofphonemes or morphemes, but only the development of such higherlevels as phrase structure indicates that this futile task need not beundertaken on lower levels.\cite{9} Similarly, we have argued that descriptionof sentence structure by constituent analysis will be unsuccessful,if pushed beyond certain limits. But only the developmentof the still more abstract level of transformations can prepare theway for the development of a simpler and more adequate techniqueof constituent analysis with narrower limits. The grammar of alanguage is a complex system with many and varied interconnectionsbetween its parts. In order to develop one part of grammarthoroughly, it is often useful, or even necessary, to have somepicture of the character of a completed system. Once again, I thinkthat the notion that syntactic theory must await the solution ofproblems of phonology and morphology is completely untenablewhether or not one is concerned with the problem of discoveryprocedures, but I think it has been nurtured by a faulty analogybetween the order of development of linguistic theory and thepresumed order of operations in discovery of grammatical structure.
\footnote{9}See N. Chomsky, M. Halle, F. Lukoff, "On accent and juncture in English,"For Roman Jakobson ('s-Gravenhage, 1 956), 65-80, for a discussion of thepossibility that considerations on all higher levels, including morphology,phrase structure, and transformations, are relevant to the selection of a phonemicanalysis.
* Some transformation in English** 7.1 After this digression, we can return to the investigation of theconsequences of adopting the transformational approach in thedescription of English syntax. Our goal is to limit the kernel in sucha way that the terminal strings underlying the kernel sentences arederived by a simple system of phrase structure and can provide thebasis from which all sentences can be derived by simple transformations:obligatory transformations in the case of the kernel,obligatory and optional transformations in the case of non-kernelsentences.
To specify a transformation explicitly we must describe theanalysis of the strings to which it applies and the structural changethat it effects on these strings.\cite{1} Thus, the passive transformationapplies to strings of the form $ NP - Aux - V - NP$ and has the effectof interchanging the two noun phrases, adding by before the finalnoun phrase, and adding $be + en$ to $Aux$ (Cf. (34)). Consider nowthe introduction of $not$ or $n't$ into the auxiliary verb phrase. Thesimplest way to describe negation is by means of a transformationwhich applies before (29ii) and introduces not or n't after the secondmorpheme of the phrase given by (28iii) if this phrase contains atleast two morphemes, or after the first morpheme of this phrase ifit contains only one. Thus this transformation $T_{not}$ operates onstrings that are analyzed into three segments in one of the followingways :
(37) (i) $NP-C-V \ldots$
(ii) $NP - C + M - \ldots$
(iii) $NP - C + have - \ldots$
(iv) $NP - C + be - \ldots$
where the symbols are as in (28), (29), and it is immaterial whatstands in place of the dots. Given a string analyzed into threesegments in one of these ways, T$_{not}$, adds not (or n't) after the secondsegment of the string. For example, applied to the terminal stringthey $\Phi$ + can - come (an instance of (37ii)) , T$_{\not}$, gives they -$\Phi$ + can + n't - come (ultimately, "they can't come"); applied tothey - $\Phi$ + have - en + come (an instance of (37iii)), it gives they -$\Phi$ + have + n't - en + come (ultimately, "they haven't come"); appliedto they - $\Phi$ + be - ing + come (an instance of (37iv)) , it givesthey - $\Phi$ + be + n't - ing + come (ultimately, "they aren't coming")The rule thus works properly when we select the last three cases of(37).
\footnote{1}For a more detailed discussion of the specification of transformations inge neral and of specific transformations, see the references cited in footnote 8, p.44.
Suppose, now, that we select an instance of (37i), i.e., a terminalstring such as
(38) John - S - come.
which would give the kernel sentence "John comes" by (29ii).Applied to (38), T$_{not}$, yields
(39) John - S + n't - come.
But we specified that T$_{not}$, applies before (29ii), which has the effectof rewriting $Af + v\, as\, v + Af\, \#$. However, we see that (29ii) does notapply at all to (39) since (39) does not now contain a sequence$Af + v$. Let us now add to the grammar the following obligatorytransformational rule which applies after (29) :
(40) $\#\, Af-> \#\, do + Af$
where do is the same element as the main verb in "John does hishomework". Cf. (29iii) for introduction of # .) What (40) states isthat do is introduced as the 'bearer' of an unaffixed affix. Applying(40) and morphological rules to (39) we derive "John doesn't come."The rules (37) and (40) now enable us to derive all and only thegrammatical forms of sentence negation.
As it stands, the transformational treatment of negation is somewhatsimpler than any alternative treatment within phrase structure.The advantage of the transformational treatment (over inclusion ofnegatives in the kernel) would become much clearer if we could findother cases in which the same formulations (i.e. , (37) and (40)) arerequired for independent reasons. But in fact there are such cases.
Consider the class of 'yes-or-no' questions such as "have theyarrived", "can they arrive:' "did they arrive". We can generate all(and only) these sentences by means of a transformation Tq thatoperates on strings with the analysis (37), and has the effect ofinterchanging the first and second segments of these strings, as thesesegments are defined in (37). We require that T$_{q}$ apply after (29i)and before (29ii). Applied to
(41) (i) $they - \Phi - arrive$
(ii) $ther - \Phi + can - arrive$
(iii) $they - \Phi + have - en + anrrive$
(iv) $they - \Phi + be - ing + arrive$
which are of the forms (37i - iv), T$_{q}$ yields the strings
(42) (i) $\Phi - they - arrive$
(ii) $\Phi + can - they - arrive$
(iii) $\Phi + have - they - en + arrive$
(iv) $\Phi + be - they - ing + arrive.$
Applying to these the obligatory rules (29ii, iii) and (40), and thenthe morphophonemic rules, we derive
(43) (i) do they arrive
(ii) can they arrive
(iii) have they arrived
(iv) are they arriving
In phonemic transcription. Had we applied the obligatory rulesdirectly to (41), with no intervening T$_{q}$, we would have derived thesentences
(44) (i) they arrive
(ii) they can arrive
(iii) they have arrived
(iv) they are arriving.
Thus (43i- iv) are the interrogative counterparts to (44i-iv).
In the case of (42i), $do$ is introduced by rule (40) as the bearer ofthe unaffixed element $\Phi$. If $C$ had been developed into $S$ or past byrule (29i), rule (40) would have introduced do as a bearer of theseelements, and we would have such sentences as "does he arrive,""did he arrive." Note that no new morphophonemic rules areneeded to account for the fact that do + $\Phi$ -> /duw/, do + S -> /ddZ/,do + past -> /did/; we need these rules anyway to account for theforms of do as a main verb. Notice also that T$_{q}$ must apply after(29i), or number will not be assigned correctly in questions.
In analyzing the auxiliary verb phrase in rules (28), (29), weconsidered $S$ to be the morpheme of the third person singular and$\Phi$ to be the morpheme affixed to the verb for all other forms of thesubject. Thus the verb has $S$ if the noun subject has $\Phi$ ("the boyarrives") and the verb has $\Phi$ if the subject has S ("the boys arrive").An alternative that we did not consider was to eliminate the zeromorpheme and to state simply that no affix occurs if the subject isnot third person singular. We see now that this alternative is notacceptable. We must have the $\Phi$ morpheme or there will be noaffix in (42i) for do to bear, and rule (40) will thus not aprly to (42i).There are many other cases where transformational analysis providescompelling reasons for or against the establisment of zero morphemes.As a negative case, consider the suggestion that intransitiveverbs be analyzed as verbs with zero object. But then thepassive transformation (34) would convert, e.g., "John - slept - $\Phi$"into the non-sentence " $\Phi$ - was slept - by John" ..... "was slept byJohn." Hence this analysis of intransitives must be rejected. Wereturn to the more general problem of the role of transformationsin determining constituent structure in \S 7.6.
The crucial fact about the question transformation T$_{q}$ is thatalmost nothing must be added to the grammar in order to describeit. Since both the subdivision of the sentence that it imposes andthe rule for appearance of do were required independently fornegation, we need only describe the inversion effected by T$_{q}$ inextending the grammar to account for yes-or-no questions. Puttingit differently, transformational analysis brings out the fact thatnegatives and interrogatives have fundamentally the same 'structure', and it can make use of this fact to simplify the descriptionof English syntax.
In treating the auxiliary verb phrase we left out of considerationforms with the heavy stressed element do as in "John does come,"etc. Suppose we set up a morpheme A of contrastive stress to whichthe following morphophonemic rule applies.
(45) . . v . . + A --. . . V . . , where * indicates extra heavy stress.
We now set up a transformation TA that imposes the samestructural analysis of strings as does Tnor (i.e., (37»), and adds A tothese strings in exactly the position where Tnor adds not or n't.Then just as Tnor yields such sentences as
(46) (i) John doesn't arrive (from John # S + n't # arrive, by (40))
(ii) John can't arrive (from John # S + can + n't # arrive)
(iii) John hasn't arrived (from John # S + have + n't # en + arrive)
T$_{A}$ yields the corresponding sentences
(47) (i) John does arrive (from John # S + A # arrive, by (40))
(ii) John can arrive (from John # S + can + A # arrive)
(iii) John has arrived (from John # S + have + A # en + arrive).
Thus TA is a transformation of 'affirmation' which affirms thesentences "John arrives", "John can arrive", "John has arrived,"etc., in exactly the same way as T$_{not}$ negates them. This is formallythe simplest solution, and it seems intuitively correct as well.There are still other instances of transformations that aredetermined by the same fundamental syntactic analysis of sentences,namely (37). Consider the transformation Tso that converts thepairs of strings of (48) into the corresponding strings of (49) :
(48) (i) John - S - arrive, 1 - $\Phi$ - arrive
(ii) John - S + can - arrive, 1 - $\Phi$ + can - arrive
(iii) John - S + have - en + arrive, 1 - $\Phi$ + have - en + arrive
(49) (i) John - S - arrive - and - so - $\Phi$ - I
(ii) John - S + can - arrive - and - so - $\Phi$ + can - I
(iii) John - S + have - en + arrive - and - so - $\Phi$ + have - I.
Applying rules (29 ii, iii), (40), and the morphophonemic rules, weultimately derive
(50) (i) John arrives and so do I
(ii) John can arrive and so can I
(iii) John has arrived and so have I.
T$_{so}$ operates on the second sentence in each pair in (48), firstreplacing the third segment of this sentence by so, and then interchangingthe first and third segment. (The element so is thus apro- VP, in much the same sense in which $he$ is a pronoun). Thetransformation T$_{so}$ combines with the conjunction transformationto give (49). While we have not described this in anywhere nearsufficient detail, it is clear that both the analysis (37) of sentencesand the rule (40) again are fundamental. Thus almost nothing newis required in the grammar to incorporate such sentences as (50),which are formed on the same underlying transformational patternas negatives, questions, and emphatic affirmatives.
There is another remarkable indication of the fundamentalcharacter of this analysis that deserves mention here. Consider thekernel sentences
(51) (i) John has a chance to live
(ii) John is my friend.
The terminal strings that underly (51) are
(52) (i) John + C + have + a + chance + to + live
(ii) John + C + be + my + friend
where have in (52i) and be in (52ii) are main verbs, not auxiliaries.Consider now how the transformations T$_{not}$, T$_{q}$ and T$_{so} apply tothese underlying strings. T$_{not}$ applies to any string of the form (37),adding not or n't between the second and the third segments, asgiven in (37). But (52i) is, in fact, an instance of both (37i) and(37iii). Hence T$_{not}$ applied to (52i) will give either (53i) or (53ii);
(53) (i) John - C + n't - have + a + chance + to + live(-. "John doesn't have a chance to live")
(ii) John - C + have + n't - a + chance + to + live(-. "John hasn't a chance to live").
But in fact both forms of (53) are grammatical. Furthermore have isthe only transitive verb for which this ambiguous negation ispossible, just as it is the only transitive verb that can be ambiguouslyanalyzed in terms of (37). That is, we have "John doesn't readbooks" but not "John readsn't books".
Similarly, T$_{q}$ applied to (52i) will give either form of (54), and T$_{so}$will give either form of (55), since these transformations are alsobased on the structural analysis (37).
(54) (i) does John have a chance to live?
(ii) has John a chance to live?
(55) (i) Bill has a chance to live and so does John.
(ii) Bill has a chance to live and so has John.
But in the case of all other transitive verbs such forms as (54ii),(55ii) are impossible. We do not have "reads John books?" or"Bill reads books and so reads John". We see, however, that theapparently irregular behavior of "have" is actually an automaticconsequence of our rules. This solves the problem raised in \S 2.3concerning the grammaticalness of (3) but not (5).
Now consider (52ii). We have not shown this, but it is in facttrue that in the simplest phrase structure grammar of English thereis never any reason for incorporating "be" into the class of verbs;i.e., it will not follow from this grammar that be is a V. Just as oneof the forms of the verb phrase is V + N P, one of the forms isbe + Predicate. Hence, even though be is not an auxiliary in (52 ii),it is nevertheless the case that of the analyses permitted by (37), only(37 iv) holds of (52 ii). Therefore the transformations T"or, Tq, andTso, applied to (52ii), yield, respectively (along with (29i)),
(56) (i) John - S + be + n't - my + friend (-+ "John isn't myfriend")
(ii) S + be - John - my + friend (-+ "is John my friend")
(iii) Bill - S + be - my + friend - and - so - S + be - John(-+ "Bill is my friend and so is John").
Again, the analogous forms (e.g., "John readsn't books," etc.) areimpossible with actual verbs. Similarly, T$_{A}$ gives "John is here"instead of "John does be here", as would be the case with actualverbs.
If we were to attempt to describe English syntax wholly in termsof phrase structure, the forms with "be" and "have" would appearas glaring and distinct exceptions. But we have just seen thatexactly these apparently exceptional forms result automaticallyfrom the simplest grammar constructed to account for the regularcases. Hence, this behavior of "be" and "have" actually turns outto be an instance of a deeper underlying regularity when we considerEnglish structure from the point of view of transformational analysis.
Notice that the occurrence of have as an auxiliary in such terminalstrings as John + C + have + en + arrive (underlying the kernelsentence "John has arrived") is not subject to the same ambiguousanalysis. This terminal string is an instance of (37 iii), but not of(37 i). That is, it can be analyzed as in (57 i), but not (57 ii).
(57) (i) John - C + have - en + arrive (NP - C + have -. . . , i.e., (37 iii))
(ii) John _- C - have + en + arrive (NP - C - V . . . , i.e., (37 i))
This string is not an instance of (37i) since this occurrence of have isnot a $V$, even though certain other occurrences of have (e.g., in(52i)) are $V$'s. The phrase structure of a terminal string is determinedfrom its derivation, by tracing segments back to node points inthe manner described in \S 4.1 . But have in (57) is not traceable toany node point labelled $V$ in the derivation of this string. (52i) isambiguously analyzable, however, since the occurrence of have in(52i) is traceable back to a $V$, and of course, is traceable back to ahave (namely, itself), in the diagram corresponding to the derivationof the string (52i). The fact that (57ii) is not a permissible analysisprevents us from deriving such non-sentences as "John doesn't havearrived" , "does John have arrived", etc.
In this section we have seen that a wide variety of apparentlydistinct phenomena all fall into place in a very simple and naturalway when we adopt the viewpoint of transformational analysis andthat, consequently, the grammar of English becomes much moresimple and orderly. This is the basic requirement that any conceptionof linguistic structure (i.e., any model for the form of grammars)must meet. I think that these considerations give ample justificationfor our earlier contention that the conceptions of phrasestructure are fundamentally inadequate and that the theory oflinguistic structure must be elaborated along the lines suggested inthis discussion of transformational analysis.
** 7.2 We can easily extend the analysis of questions given above toinclude such interrogatives as
(58) (i) what did John eat
(ii) who ate an apple
which do not receive yes-or-no answers. The simplest way toincorporate this class of sentences into the grammar is by setting upa new optional transformation T w which operates on any string ofthe form
(59) $X - NP - Y$
where X and Y stands for any string (including, in particular, the'null' string - i.e., the first or third position may be empty). T$_{w}$then operates in two steps:
(60) (i) T$_{wl}$ converts the string of the form $X - NP - Y$ into thecorresponding string of the form $NP - X - Y$; i.e., itinverts the first and second segments of (59). It thus hasthe same transformational effect as T$_{q}$ (cf. (41)-(42)).
(ii) T$_{w2}$ converts the resulting string $NP - X - Y$ into $who - X - Y$ if $NP$ is an animate $NP$ or into $what - X - Y$ if $NP$ is inanimate.\cite{2}
We now require that T$_{w}$ can apply only to strings to which T$_{q}$ hasalready applied. We specified that T$_{q}$ must a pply after (29i) andbefore (29ii). T$_{w}$ applies after T$_{q}$ and before (29ii), and it is conditionalupon T$_{q}$ in the sense that it can only apply to forms givenby T$_{q}$. This conditional dependence among transformations is ageneralization of the distinction between obligatory and optionaltransformations which we can easily build into the grammar, andwhich proves essential. The terminal string underlying both (58i)and (58ii) (as well as (62), (64)) is
(61) John - C - eat + an + apple ($NP - C - V \ldots$),
where the dashes indicate the analysis imposed by T$_{q}$. Thus (61) isa case of (37i), as indicated. If we were to apply only obligatorytransformations to (61), choosing past in developing C by (29 i), wewould derive
(62) # John # eat + past # an # apple # (-+ "John ate an apple")
If we apply (29i) and Tq to (61), we derive
(63) past - John - eat + an + apple,
where C is taken as past. If we were now to apply (40) to (63),introducing do as the bearer of past, we would have the simpleinterrogative
(64) did John eat an apple
If we apply T$_{w}$ to (63), however, we derive first (65), by T$_{w1}$, andthen (66), by T$_{w2}$.
(65) John - past - eat + an + apple
(66) who - past - eat + an + apple.
Rule (29ii) and the morphophonemic rules then convert (66) into(58ii). To form (58ii), then, we apply first T$_{q}$ and then T$_{w}$ to theterminal string (61) that underlies the kernel sentence (62). Notethat in trus case T$_{w1}$ simply undoes the effect of T$_{q}$, which explainsthe absence of inversion in (58ii).
\footnote{2}More simply; we can limit application of T w to strings X - NP - Y whereNP is he, him, or it, and we can define T �2 as the transformation that convertsany string Z into wh + Z, where wh is a morpheme. In the morphophonemics ofEnglish we shall have rules : wh + he ..... /huw/, wh + him ..... /huwm/, wh + it ...../wat/.

To apply T$_{w}$ to a string, we first select a noun phrase and theninvert this noun phrase with the string that precedes it. In forming(58ii), we applied T$_w$ to (63), choosing the noun phrase John.Suppose now that we apply T$_w$ to (63), choosing the noun phrasean + apple. Thus for the purposes of this transformation we nowanalyze (63) as
(67) $past + John + eat - an + apple$,
a string of the form (59), where $Y$ in this case is null. Applying T$_w$to (67) we derive first (68), by T$_{w1}$ , and then (69), by T$_{w1}$
(68) $an + apple - past + John + eat$
(69) $what - past + John + eat$.
(29ii) does not now apply to (69), just as it did not apply to (39) or(42i), since (69) does not contain a substring of the form $Af + v$.Hence (40) applies to (69), introducing do as a bearer of themorpheme past. Applying the remaining rules, we finally derive(58i).
T$_w$ as formulated in (59)-(60) will also account for all such whquestionsas "what will he eat", "what has he been eating". It caneasily be extended to cover interrogatives like "what book did heread", etc.
Notice that T$_{w1}$ as defined in (60i) carries out the same transformationas does T$_q$; that is, it inverts the first two segments of thestring to which it applies. We have not discussed the effect oftransformations on intonation. Suppose that we set up two fundamentalsentence intonations: falling intonations, which we associatewith kernel sentences, and rising intonations, which we associatewith yes-or-no questions. Then the effect of T$_q$ is in part toconvert the intonation from one of these to the other; hence, in thecase of (64), to convert a falling intonation into a rising one. But wehave seen that T$_{w1}$ applies only after T$_q$, and that its transformationaleffect is the same as that of T$_q$. Hence T$_{w1}$ will convert the risingintonation back into a falling one. It seems reasonable to put thisforth as an explanation for the fact that the interrogatives (58i-ii)normally have the falling intonation of declaratives. There aremany problems in extending our discussion to intonational phenomenaand this remark is too sketchy to carry much weight, but itdoes suggest that such an extension may be fruitful.
To summarize, we see that the four sentences
(70) (i) John ate an apple ( = (62))
(ii) did John eat an apple ( = (64))
(iii) what did John eat ( = (58i))
(iv) who ate an apple ( = (58ii))
are all derived from the underlying terminal string (61). (70i) is akernel sentence, since only obligatory transformations enter into its'transformational history.' (70ii) is formed from (61) by applyingT$_q$. (70iii) and (70iv) are even more remote from the kernel, sincethey are formed from (61) by applying first T$_q$ and then T$_w$. Weshall refer to this analysis briefly in \S 8.2.
** 7.3 In \S 5.3 we mentioned that there are certain noun phrases ofthe form $to + VP$, $ing + VP$ ("to prove that theorem," "proving thattheorem"- cf. (32)-(33)). Among these we will have such phrases as"to be cheated," "being cheated", which are derived from passives.But passives have been deleted from the kernel. Hence nounphrases of the type $to + VP$, $ing + NP$ can no longer be introducedwithin the kernel grammar by such rules as (33). They musttherefore be introduced by a 'nominalizing transformation' whichconverts a sentence of the form $NP - VP$ into a noun phrase of theform $to + VP$ or $ing + VP$.\cite{3} We shall not go into the structure ofthis very interesting and ramified set of nominalizing transformationsexcept to sketch briefly a transformational explanation for aproblem raised in \S 2.3.
One of the nominalizing transformations will be the transformationT$_{adj}$ which operates on any string of the form
(71) $T - N - is - Adj$ (i.e., article - noun - is - adjective)
and converts it into the corresponding noun phrase of the form$T + Adj + N$. Thus, it converts "the boy is tall" into " the tall boy,"etc. It is not difficult to show that this transformation simplifies thegrammar considerably, and that it must go in this, not the oppositedirection. When we formulate this transformation properly, wefind that it enables us to drop all adjective-noun combinations fromthe kernel, reintrod ucing them by T$_{adj}$.
In the phrase structure grammar we have a rule
(72) $Adj -> old, tall, \ldots$
which lists all of the elements that can occur in the kernel sentencesof the form ( 71). Words like "sleeping", however, will not be givenin this list, even though we have such sentences as
(73) the child is sleeping.
The reason for this is that even when "sleeping" is not listed in (72),(73) is generate by the transformation (29ii) (that carries $Af + v$into $v + Af\, \#$ ) form the underlying terminal string
(74) $the + child + C + be - ing - sleep$,
where $be + ing$ is part of the auxiliary verb (cf. (28iii)). Alongside of(73), we have such sentences as "the child will sleep," "the childsleeps," etc., with different choices for the auxiliary verb.Such words as "interesting", however, will have to be given in thelist (73). In such sentences as
(75) the book is interesting,
"interesting" is an $Adj$, not part of the $Verb$, as can be seen from thefact that we do not have "the book will interest," "the bookinterests," etc.
\footnote{3} This nominalizing transformation will be given as a generalized transfonnationsuch as (26). It will operate on a pair sentences, one of which itconverts from $NP - VP$ into $to + VP$ (or $ing + VP$), which it then substitutes foran $NP$ of the other sentence. See my {\it The logical structure of linguistic theory andTransformational analysis} for a detailed discussion. - For a fuller and moreadequate analysis of the material in this subsection, see my "A transformationalapproach to syntax," {\it Proceedings of the University of Texas Symposium of 1958}(to appear).

An independent support for this analysis of "interesting" and"sleeping" comes from the behavior of "very," etc., which can occurwith certain adjectives, but not others. The simplest way to accountfor "very" is to put into the phrase structure grammar the rule
(76) $Adj -> very + Adj$.
"very" can appear in (75), and in general with "interesting"; but itcannot appear in (73) or with other occurrences of "sleeping."Hence, if we wish to preserve the simplest analysis of "very," wemust list "interesting" but not "sleeping" in (72) as an $Adj$.

We have not discussed the manner in which transformationsimpose constituent structure, although we have indicated that thisis necessary; in particular, so that transformations can be compounded.One of the general conditions on derived constituentstructure will be the following :
(77) If $X$ is a $Z$ in the phrase structure grammar, and a string $Y$formed by a transformation is of the same structural formas $X$, then $Y$ is also a $Z$.
In particular, even when passives are deleted from the kernel we willwant to say that the $by$-phrase (as in "the food was eaten - by theman") is a prepositional phrase ($PP$) in the passive sentence. (77)permits this, since we know from the kernel grammar that $by + NP$is a $PP$. (77) is not stated with sufficient accuracy, but it can beelaborated as one of a set of conditions on derived constituentstructure.
But now consider (73). The word "sleeping" is formed by transformation(i.e., (29ii)) and it is of the same form as "interesting"(i.e., it is a $V + ing$), which, as we know from the phrase structuregrammar, is an $Adj$. Hence, by (77), "sleeping" is also an $Adj$ in thetransform (73). But this means that (73) can be analyzed as a stringof the form (71) so that T$_{Adj}$ applies to it, forming the naun phrase
(78) the sleeping child
just as it forms " the interesting book" from (75). Thus even though"sleeping" is excluded from (72), it will appear as an adjectivemodifying nouns.
This analysis of adjectives (which is all that we are required togive to account for the actually occurring sentences) will notintroduce the word "sleeping," however, into all the adjectivepositions of such words as "interesting" which remained in thekernel. For example, it will never introduce "sleeping" into thecontext "very -." Since "very" never modifies verbs, "very" willnot appear in (74) or (73), and all occurences of "sleeping" as amodifier are derived from its occurrence as a verb in (74), etc.Similarly, there will be phrase structure rules that analyze the verbphrase into
(79) $Aux + seem + Adj$
just as other rules analyze $VP$ into $Aux + V + NP$, $Aux + be + Adj$.etc. But "sleeping" will never be introduced into the context"seems -" by this grammar, which is apparently the simplest oneconstructible for the actually occurring sentences.When we develop this sketchy argument more carefully, we reachthe conclusion that the simplest transformational grammar for theoccurring sentences will exclude (80) while generating (81).
(80) (i) the child seems sleeping
(ii) the very sleeping child
(81) (i) the book seems interesting
(ii) the very interesting book.
We see, then, that the apparently arbitrary distinctions noted in\S 2.3 between (3) ( = "have you a book on modern music?") and(4) ( = (81i)) on the one hand, and (5) ( = "read you a book onmodern music ?") and (6) ( = (80i)) on the other, have a clear structuralorigin, and are really instances of higher level regularity in thesense that they are consequences of the simplest transformationalgrammar. In other words, certain linguistic behavior that seemsunmotivated and inexplicable in terms of phrase structure appearssimple and systematic when we adopt the transformational point ofview. To use the terminology of \S 2.2, if a speaker were to projecthis finite linguistic experience by using phrase structure and transformationsin the simplest possible way, consistent with his experience,he would include (3) and (4) as grammatical while rejecting(5) and (6).
** 7.4 In (28), \S 5.3, we analyzed the element $Verb$ into $Aux + V$, andthen simply listed the verbal roots of the class $V$. There are, however,a large number of productive subscontructions of $V$ thatdeserve some mention, since they bring to light some basic pointsin a rather clear way. Consider first such verb + particle ($V + Prt$)constructions as "bring in," "call up," "drive away." We can havesuch forms as (82) but not (83).
(82) (i) the police brought in the criminal
(ii) the police brought the criminal in
(iii) the police brought him in
(83) the police brought in him.
We know that discontinuous elements cannot be handled readilywithin the phrase structure grammar. Hence the most natural wayof analyzing these constructions is to add to (28ii) the followingpossibility:
(84) $V -> V_1 + Prt$
along with a set of supplementary rules to indicate which $V_1$ can gowith which $Prt$. To allow for the possibility of (82ii) we set up anoptional transformation T$^{op}_{sep}$ which operates on strings with thestructural analysis
(85) $X - V_1 - Prt - NP$
and has the effect of interchanging the third and fourth segments ofthe string to which it applies. It thus carries (82i) into (82ii). Toprovide for (82iii) while excluding (83), we must indicate that thistransformation is obligatory when the $NP$ object is a pronoun($Pron$). Equivalently, we can set up an obligatory transformationT$^{ob}_{sep}$ which has the same structural effect as T$^{op}_{sep}$ but which operateson strings with the structural analysis
(86) $X - V_1 - Prt - Pron$
We know that the passive transformation operates on any string ofthe form $NP - Verb - NP$. If we specify that the passive transformationapplies before T$^{ob}_{sep}$ or T$^{op}_{sep}$ then it will form the passives
(87) (i) the criminal was brought in by the police
(ii) he was brought in by the police
from (82), as it should.
Further investigation of the verb phrase shows that there is ageneral verb + complement ($V + Comp$) construction that behavesvery much like the verb + particle construction just discussed.Consider the sentences
(88) everyone in the lab considers John incompetent
(89) John is considered incompetent by everyone in the lab.
If we wish to derive (89) from (88) by the passive transformation wemust analyze (88) into the structure $NP_1- Verb - NP_2, where $NP_1 = everyone + in + the + lab$ and $NP_2 = John$. That is, we must apply thepassive not to (88), but to a terminal string (90) that underlies (88):
(90) everyone in the lab - considers incompetent - John.
We can now form (88) from (90) by a transformation analogous toT$^{ob}_{sep}$. Suppose that we add to the phrase structure grammar the rule(91), alongside (84).
(91) $V -> V_a + Comp$
We now extend T$^{ob}_{sep}$ permitting it to apply to strings of the form(92) as well as to strings of the form (86), as before.
(92) $X - V_a - Comp - NP$.
This revised transformation T$^{ob}_{sep}$ will convert (90) into (88). Thus,the treatment of the verb + complement and verb + particle constructionsare quite similar. The former, in particular, is an extremelywell-developed construction in English.\cite{4}
\footnote{4}Further study shows that most of the verb + complement forms introducedby rule (91 ) should themselves be excluded from the kernel and derived transformationallyfrom "John is incompetent," etc. But this is a complex matterthat requires a much more detailed development of transformational theorythan we can give here. Cf. my {\it The logical structure of linguistic theory, Transformationalanalysis} and "A transformational approach to syntax".
There are several other features of these constructions that we have passedover far too briefly. It is not at all clear that this is an obligatory transformation.With long and complex objects we can have, e.g., "they consider incompetentanyone who is unable to $\ldots$" Hence we might extend T$^{op}_{sep}$, rather than T$^{ob}_{sep}$, totake care of this case. It is interesting to study those features of the grammaticalobject that necessitate or preclude this transformation. Much more than lengthis involved. There are also other possibilities for the passive that we shall notconsider here, for lack of space, though they make an interesting study.
** 7.5 We have barely sketched the justification for the particularform of each of the transformations that we have discussed, thoughit is very important to study the question of the uniqueness of thissystem. I think it can be shown that in each of the cases consideredabove, and in many other cases, there are very clear and easilygeneralizable considerations of simplicity that determine which setof sentences belong to the kernel and what sorts of transformationsare required to account for the non-kernel sentences. As a paradigmaticinstance, we shall briefly review the status of the passivetransformation.
In \S 5.4 we showed that the grammar is much more complex if itcontains both actives and passives in the kernel than if the passivesare deleted and reintroduced by a transformation that interchangesthe subject and object of the active, and replaces the verb $V$ by$is + V + en + by$. Two questions about uniqueness immediatelysuggest themselves. First, we ask whether it is necessary to inter-change the noun phrases to form the passive. Second, we askwhether passives could have been chosen as the kernel, and activesderived from them by an 'active' transformation.
Consider first the question of the interchange of subject andobject. Is this interchange necessary, or could we describe thepassive transformation as having the following effect :
(93) $NP_1 - Aux - V - NP_2$ is rewritten $NP_1 - Aux + be + en V- by + NP_2$.
In particular, the passive of "John loves Mary" would be "John isloved by Mary."
In \S 5.4 we argued against (93) and in favor of inversion on thebasis of the fact that we have such sentences as (94) but not (95).
(94) (i) John admires sincerity - sincerity is admired by John
(ii) John plays golf - golf is played by John
(iii) sincerity frightens John - John is frightened by sincerity
(95) (i) sincerity admires John - John is admired by sincerity
(ii) golf plays John - John is played by golf
(iii) John frightens sincerity - sincerity is frightened by John.
We pointed out, however, that this approach requires that a notionof "degree of grammaticalness" be developed to support thisdistinction. I believe that this approach is correct, and that there isa clear sense in which the sentences of (94) are more grammaticalthan those of (95), which are themselves more grammatical than"sincerity admires eat," etc. Any grammar that distinguishesabstract from proper nouns would be subtle enough to characterizethe difference between (94i, iii) and (95i, iii), for example, and surelylinguistic theory must provide the means for this distinction. However,since we have not gone into the question of category analysisin this discussion, it is interesting to show that there is even astronger argument against (93). In fact, any grammar that candistinguish singular from plural is sufficiently powerful to enable usto prove that the passive requires inversion of noun phrases.
To see this, consider the verb + complement construction discussedin \S 7.4. Alongside (88), (89) we have such sentences as :
(96) all the people in the lab consider John a fool
(97) John is considered a fool by all the people in the lab.
In \S 7.4 we saw that (96) is formed by the transformation T$^{ob}_{sep}$ fromthe underlying string
(98) all the people in the lab - consider a fool - John ($NP - Verb -NP$) ,
with the $Verb$ "consider a fool" being an instance of (91). Wealso saw that the passive transformation applies directly to (98). Ifthe passive interchanges subject and object, it will correctly form(97) from (98) as the passive of (96). If, however, we take (93) as thedefinition of the passive, we will derive the non-sentence.
(99) all the people in the lab are considered a fool by John
by application of this transformation to (98).
The point is that we have found a verb - namely, "consider afool" - which must agree in number both with its subject and itsobject.\cite{5} Such verbs prove quite conclusively that the passive mustbe based on an inversion of subject and object.
\footnote{5}The agreement between "a fool" and "John" in (98) is of course onesupport for the futher transformational analysis of the verb + complement +noun phrase constructions mentioned ih footnote 4 on p. 77.
Consider now the question of whether passives could be taken asthe kernel sentences instead of actives. It is quite easy to see thatthis proposal leads to a much more complex grammar. With activesas kernel sentences, the phrase structure grammar will include (28)with $be + en$ dropped from (28iii). But if passives are taken as kernelsentences, $be + en$ will have to be listed in (28iii), along with allthe other forms of the auxiliary, and we will have to add special rulesindicating that if $V$ is intransitive, it cannot have the auxiliary$be + en$ (i.e., we cannot have "is occurred"), whereas if $V$ is transitiveit must have $be + en$ (i.e., we cannot have "lunch eatsby John"). Comparing the two alternatives, there is no doubt asto relative complexity; and we are forced to take actives, notpassives, as the kernel sentences.
Notice that if passives were chosen as kernel sentences instead ofactives we would run into certain difficulties of quite a different sort.The active transformation would have to apply to strings of the form
(100) $NP_1 - Aux + be + en - V - by + NP_2$,
converting them to $NP_2 - Aux - V - NP_1$. For example, it wouldconvert
(101) the wine was drunk by the guests
into "the guests drank the wine," where "drunk" in (101) originatesfrom $en + drink$. But there is also an adjective "drunk" that must belisted in (72) along with "old," "interesting," etc., since we have"he is very drunk," "he seems drunk," etc. (cf. \S 7.3), and thisadjective will also originate from $en + drink$. It thus appears that inthe simplest system of phrase structure for English, the sentence
(102) John was drunk by midnight
is also based on an underlying terminal string that can be analyzedin accordance with (100). In other words, there is no structural wayto differentiate properly between (101) and (102), if both are takenas kernel sentences. But application of the 'active' transformationto (102) does not give a grammatical sentence.
When we actually try to set up, for English, the simplest grammarthat contains a phrase structure and transformational part, we findthat the kernel consist of simple, declarative, active sentences (infact, probably a finite number of these), and that all other sentencescan be described more simply as transforms. Each transformationthat I have investigated can be shown to be irreversible in the sensethat it is much easier to carry out the transformation in one directionthan in the other, just as in the case of the passive transformationdiscussed above. This fact may account for the traditional practiceof grammarians, who customarily begin the grammar of English,for example, with the study of simple 'actor-action' sentences andsimple grammatical relations such as subject-predicate or verbobject.No one would seriously begin the study of English constituentstructure with such a sentence as "whom have they nominated,"attempting to analyze it into two parts, etc.; and while somevery detailed considerations of English structure (e.g., reference [33])do not mention interrogatives, none fails to include simple declara-tives. Transformational analysis provides a rather simple explanationfor this assymmetry (which is otherwise formally unmotivated)on the assumption that grammarians have been acting on the basisof a correct intuition about the language.\cite{6}
\footnote{6}In determining which of two related forms is more central, we are thusfollowing the reasoning outlined by Bloomfield for morphology: "$\ldots$ when formsare partially similar, there may be a question as to which one we had better takeas the underlying form $\ldots$ the structure of the language may decide this questionfor us, since, taking it one way, we get an unduly complicated description, andtaking it the other way, a relatively simple one," ({\it Language} [New York,1933),p. 218). Bloomfield continues by pointing out that "this same considerationoften leads us to set up an artificial underlying form." We have also found thisinsight useful in transformational analysis, as, e.g., when we set up the terminalstring $John - C - have + en - be + ing - read$ underlying the kernel sentence"John has been reading."

** 7.6 One other point deserves some mention before we leave thetopic of English transformations. At the outset of \S 5 we noted thatthe rule for conjunction provides a useful criterion for constituentanalysis in the sense that this rule is greatly simplified if constituentsare set up in a certain way. Now we are interpreting this rule as atransformation. There are many other cases in which the behaviorof a sentence under transformations provides valuable, even compellingevidence as to its constituent structure.Consider for example the pair of sentences
(103) (i) John knew the boy studying in the library.
(ii) John found the boy studying in the library.
It is intuitively obvious that these sentences have different grammaticalstructure (this becomes clear, for example, when we attemptto add "not running around in the streets" to (103)), but I do notbelieve that within the level of phrase structure grounds can befound for analyzing them into different constituents. The simplestanalysis in both cases is as $NP - Verb - NP - ing + VP$. But considerthe behavior of these sentences under the passive transformation.We have the sentences (104) but not (105).\site{7}
(104) (i) the boy studying in the library was known (by John)
(ii) the boy studying in the library was found (by John)
(iii) the boy was found studying in the library (by John)
(105) the boy was known studying in the library (by John)
The passive transformation applies only to sentences of the form$NP - Verb - NP$. Hence, to yield (104ii), (103ii) must be analyzableas
(106) John - found - the boy studying in the library,
with the noun phrase object "the boy studying in the library,"(103i) will have a corresponding analysis, since we have the passive(104i).
\footnote{7}The sentences of (104) without the parenthesized expression are formed bya second 'elliptical' transformation that converts e.g., "the boy was seen byJohn" into "the boy was seen."

But (103ii) also has the passive (104iii). From this we learn that(103ii) is a case of the verb + complement construction studied in\S 7.4; i.e., that it is derived by the transformation T$^{ob}_{sep} from theunderlying string
(107) John - found studying in the library - the boy,
with the verb "found" and the complement "studying in thelibrary." The passive transformation will convert (107) into (104iii),just as it converts (90) into (89). (103i), however, is not a transformof the string "John - knew studying in the library - the boy" (thesame form as (107)), since (105) is not a grammatical sentence.By studying the grammatical passives, then, we determine that"John found the boy studying in the library" ( = (103ii)) is analyzableambiguously as either $NP - Verb - NP$, with the object "the boystudying in the library," or as $NP - Aux + V - NP - Comp$, atransform of the string (107 which has the complex Verb "foundstudying in the library." "John knew the boy studying in thelibrary" ( = (103i)) , however, has only the first of these analyses.The resulting description of (103) seems quite in accord withintuition.
As another example of a similar type, consider the sentence
(108) John came home.
Although "John" and "home" are $NP$'s, and "came" is a $Verb$,investigation of the effect of transformations on (108) shows that itcannot be analyzed as a case of $NP - Verb - NP$. We cannot have"home was come by John" under the passive transformation, or"what did John come" under the question transformation T$_w$. Wemust therefore analyze (108) in some other way (if we are not tocomplicate unduly the description of these transformations),perhaps as $NP - Verb - Adverb$. Apart from such considerationsas these, there do not appear to be very strong reasons for denyingto (108) the completely counterintuitive analysis $NP - Verb - NP$,with "home" the object of "came".
I think it is fair to say that a significant number of the basiccriteria for determining constituent structure are actually transformational.The general principle is this: if we have a transformationthat simplifies the grammar and leads from sentences to sentencesin a large number of cases (i.e., a transformation under whichthe set of grammatical sentences is very nearly closed), then weattempt to assign constituent structure to sentences in such a waythat this transformation always leads to grammatical sentences, thussimplifying the grammar even further.
The reader will perhaps have noted a certain circularity or evenapparent inconsistency in our approach. We define such transformationsas the passive in terms of particular phrase structureanalyses, and we then consider the behavior of sentences underthese transformations in determining how to assign phrase structureto these sentences. In \S 7.5 we used the fact that "John was drunkby midnight" ( = (102)) does not have a corresponding 'active' as anargument against setting up a passive-to-active transformation. In\S 7.6 we have used the fact that "John came home" ( = (108)) doesnot have a passive as an argument against assigning to it the constituentstructure $NP - Verb - NP$. However, if the argument istraced carefully in each case it will be clear that there is no circularityor inconsistency. In each case our sole concern has been to decreasethe complexity of the grammar, and we have tried to show that theproposed analysis is clearly simpler than the rejected alternatives.In some cases the grammar becomes simpler if we reject a certaintransformation: in some cases reassignment of constituent structureis preferable. We have thus been following the course outlined in\S 6. Making use of phrase structure and transformations, we aretrying to construct a grammar of English that will be simpler thanany proposed alternative; and we are giving no thought to thequestion of how one might actually arrive at this grammar in somemechanical way from an English corpus, no matter how extensive.Our weaker goal of evaluation instead of discovery eliminates anyfear of vicious circularity in the cases discussed above. The intuitivecorrespondences and explanations of apparent irregularities seemto me to offer important evidence for the correctness of the approachwe have been following. Cf. \S 8.

* 8 THE EXPLANATORY POWER OF LINGUISTIC THEORY** 8.1 So far we have considered the linguist's task to be that ofprod ucing a device of some sort (called a grammar) for generatingall and only the sentences of a language, which we have assumedwere somehow given in advance. We have seen that this conceptionof the linguist's activities leads us naturally to describe languages interms of a set of levels of representation, some of which are quiteabstract and non-trivial. In particular, it leads us to establish phrasestructure and transformational structure as distinct levels ofrepresentation for grammatical sentences. We shall now proceed toformulate the linguist's goals in quite different and independentterms which, however, lead to very similar notions of linguisticstructure.
There are many facts about language and linguistic behavior thatrequire explanation beyond the fact that such and such a string(which no one may ever have produced) is or is not a sentence. It isreasonable to expect grammars to provide explanations for someof these facts. For example, for many English speakers the phonemesequence /'dneym/ can be understood ambiguously as either "aname" or "an aim". If our grammar were a one-level system dealingonly with phonemes, we would have no explanation for this fact.But when we develop the level of morphological representation, wefind that, for quite independent reasons, we are forced to set upmorphemes "a", "an", "aim" and "name", associated with thephonemic shapes /'d/, /'dn/, /eym/ and /neym/. Hence, as an automaticconsequence of the attempt to set up the morphology in thesimplest possible way we find that the phoneme sequence /'dneym/ isambiguously represented on the morphological level. In general ,we say that we have a case of constructional homonymity when acertain phoneme sequence is analyzed in more than one way onsome level. This suggests a criterion of adequacy for grammarsWe can test the adequacy of a given grammar by asking whether ornot each case of constructional homonymity is a real case of ambiguityand each case of the proper kind of ambiguity is actually acase of constructional homonymity.\cite{l} More generally, if a certainconception of the form of grammar leads to a grammar of a givenlanguage that fails this test, we may question the adequacy of thisconception and the linguistic theory that underlies it. Thus, aperfectly good argument for the establishment of a level of morphologyis that this will account for the otherwise unexplainedambiguity of /'dneym/.
\footnote{1}Obviously, not all kinds of ambiguity will be analyzable in syntactic terms.For example, we would not expect a grammar to explain the referential ambiguityof "son"-"sun", "light" (in color, weight), etc.
In his "Two models of grammatical description," Linguistics Today, Word10.210-33 (1954), Hockett uses notions of structural ambiguity to demonstratethe independence of various linguistic notions in a manner very similar to whatwe are suggesting here.

We have a case of constructional homonymity when some phonemesequence is ambiguously represented. Suppose that on somelevel two distinct phoneme sequences are similarly or identicallyanalyzed. We expect that these sequences should somehow be'understood' in a similar manner, just as cases of dual representationare 'understood' in more than one way. For example, thesentences
(109) (i) John played tennis
(ii) my friend likes music
are quite distinct on phonemic and morphemic levels. But on thelevel of phrase structure they are both represented as $NP - Verb - NP$;correspondingly, it is evident that in some sense they are similarlyunderstood. This fact could not be explained in terms of a grammarthat did not go beyond the level words or morphemes, andsuch instances offer a motivation for establishing the level of phrasestructure that is quite independent of that advanced in \S 3. Notethat considerations of structural ambiguity can also be broughtforth as a motivation for establishing a level of phrase structure.Such expressions as "old men and women" and "they are flyingplanes" (i.e., "those specks on the horizon are $\ldots$", "my friendsare $\ldots$") are evidently ambiguous, and in fact they are ambiguouslyanalyzed on the level of phrase structure, though not on any lowerlevel. Recall that the analysis of an expression on the level ofphrase structure is provided not by a single string but by a diagramsuch as (15) or, equivalently, by a certain set of representing strings.\cite{2}
What we are suggesting is that the notion of "understanding asentence" be explained in part in terms of the notion of "linguisticlevel". To understand a sentence, then, it is first necessary toreconstruct its analysis on each linguistic level; and we can test theadequacy of a given set of abstract linguistic levels by asking whetheror not grammars formulated in terms of these levels enable us toprovide a satisfactory analysis of the notion of "understanding."Cases of higher level similarity of representation and higher leveldissimilarity (constructional homonymity) are simply the extremecases which, if this framework is accepted, prove the existence ofhigher levels. In general, we cannot understand any sentence fullyunless we know at least how it is analyzed on all levels, includingsuch higher levels as phrase structure, and, as we shall see, transformationalstructure.
\footnote{2}That is, by what is called a "phrase marker" in my The logical structure oflinguistic theory and "Three models for the description of language" (above,p.22, fn. 1). See "Three models$\ldots$" for discussion of the constructionalhomonymity of "they are flying planes" within a phrase structure grammar.When we adjoin a transformational grammar to the phrase structure grammar,this sentence is, however, an example of transf ormational ambiguity, notconstructional homonymity within phrase structure. In fact, it is not clear thatthere are any cases of constructional homonymity purely within the level ofphrase structure once a transformational grammar is developed.

We were able to show the inadequacy of a theory of linguisticstructure that stopped short of phrase structure by exhibiting casesof ambiguity and similarity of understanding that were unexplainedon lower levels. But it turns out that there is still a large residue of unexplainedcases even after the level of phrase structure is establishedand applied to English. Analysis of these cases demonstrates thenecessity for a still 'higher' level of transformational analysis in amanner independent of \S\S 5, 7. I shall mention only a few representativeinstances.
** 8.2 In \S 7.6 we came across ar example of a sentence (i.e., "I foundthe boy studying in the library" = (103 ii)) whose ambiguity ofrepresentation could not be demonstrated without bringing transformationalcriteria to bear. We found that under one interpretationthis sentence was a transform under T$^{ob{_{sep}$ of "I - foundstudying in the library - the boy," and that under another interpretationit was analyzed into an $NP - Verb - NP$ construction withthe object "the boy studying in the library." Further transformationalanalysis would show that in both cases the sentence is atransform of the pair of terminal strings that underlie the simplekernel sentences
(110) (i) I found the boy
(ii) the boy is studying in the library.
Hence this is an interesting case of a sentence whose ambiguity isthe result of alternative transformational developments from thesame kernel strings. This is quite a complicated example, however,requiring a fairly detailed study of the way in which transformationsassign constituent structure, and simpler examples of ambiguitywith a transformational origin are not hard to find.
Consider the phrase (111), which can be understood ambiguouslywith "hunters" as the subject, analogously to (112i), or as theobject, analogously to (112ii).
(111) the shooting of the hunters
(112) (i) the growling of lions
(ii) the raising of flowers.
On the level of phrase structure there is no good way to explain thisambiguity; all of these phrases are represented as $the - V + ing -of + NP$.\cite{3} In transformational terms, however, there is a clear andautomatic explanation. Careful analysis of English shows that wecan simplify the grammar if we strike such phrases as (111) and (112)out of the kernel and reintroduce them by transformation. Toaccount for such phrases as (112i), we will set up a transformationthat carries any sentence of the form $NP - C - V$ into the correspondingphrase of the form $the - V + ing - of + NP$; and thistransformation will be designed in such a way that the result is an$NP$.\cite{4} To account for (112ii), we will set up a transformation whichcarries any sentence of the form $NP_1 - C - V - NP_2$ into the corresponding$NP$ of the form $the - V + ing - of+ NP_2$. Thus the first ofthese transformations will carry "lions growl" into "the growling oflions," and the second will carry " John raises flowers" into "theraising of flowers." But both "the hunters shoot" and "they shootthe hunters" are kernel sentences. Hence (111) = "the shooting ofthe hunters" will have two distinct transformational origins; it willbe ambiguously represented on the transformational level. Theambiguity of the grammatical relation in (111) is a consequence ofthe fact that the relation of "shoot" to "hunters" differs in the twounderlying kernel sentences. We do not have this ambiguity in (112),since neither "they growl lions" nor "flowers raise" are grammaticalkernel sentences.
\footnote{3}It is true that (111) may be represented ambiguously with shoot takeneither as a transitive or an intransitive verb, but the essential fact here is that thegrammatical relation in (111) is ambiguous (i.e., "hunters" may be subject orobject). Grammatical relations can be defined within phrase structure in termsof the shape of the diagrams (15), etc. But in these terms there will be no groundsfor the assertion that either the subject·verb or the verb-object relation is to befound in (111). If we analyze verbs into three classes, transitive, intransitive andeither transitive or intransitive, then even this (in itself insufficient) distinctiondisappears.
\footnote{4} Cf. footnote 3 on p.72.
Similarly, consider such pairs as
(113) (i) the picture was painted by a new technique
(ii) the picture was painted by a real artist.
These sentences are understood quite differently, though identicallyrepresented as $NP - was + Verb + en - by + NP$ on the level ofphrase structure. But their transformational history is quite different.(113ii) is the passive of "a real artist painted the picture". (113i) isformed from, e.g., "John painted the picture by a new technique"by a double transformation; first the passive, then the ellipticaltransformation (mentioned in fn.7 on p.81) that drops the 'agent'in the passive. An absolute homonym on the model of (113) is nothard to find. For example,
(114) John was frightened by the new methods.
may mean either that John is a conservative - new methods frightenhim; or that new methods of frightening people were used tofrighten John (an interpretation that would be the more normal oneif "being" were inserted after "was"). On the transformationallevel, (114) has both the analysis of (113i) and (113ii), whichaccounts for its ambiguity.
** 8.3 We can complete the argument by presenting an example ofthe opposite extreme; namely, a case of sentences which are understoodin a similar manner, though they are quite distinct in phrasestructure and lower level representation. Consider the followingsentences, discussed in \S 7.2.
(115) (i) John ate an apple - declarative
(ii) did John eat an apple- yes-or-no-questiOn ) 
(iii) what did John eat  - interrogative.wh-question
(IV) who ate an apple
It is intuitively obvious that (115) contains two types of sentences,declaratives (115i) and interrogatives (115ii-iv). Furthermore, theinterrogatives are intuitively subdivided into two types, the yes-or-no-question(115ii), and the $wh$-questions (115iii, iv). It is difficult,however, to find a formal basis for this classification that is notarbitrary and ad hoc. If, for example, we classify sentences by their'normal' intonation, then (115i), (115iii) and (115iv), with thenormal declarative (falling) intonation, will be opposed to (115ii),with rising intonation. If we classify sentences on the basis of wordorder, then (115i) and (l15iv), with normal $NP - Verb - NP$ order,will be opposed to (115ii) and (115iii), which have inversion ofsubject and auxiliary. Nevertheless, any grammar of English willclassify these sentences in the manner indicated in (115), and anyspeaker of English will understand these sentences according to thispattern. Certainly a linguistic theory that fails to provide groundsfor this classification must be judged inadequate.
The representation of a string on the level of transformations isgiven by the terminal string (or strings) form which it originates andthe sequence of transformations by which it is derived from thisunderlying string. In \S\S 7.1 -2 we came to the following conclusionsabout the sentences (115) ( = (70)). Each of these sentences originatesfrom the underlying terminal string.
(116) $John C - eat + an + apple$ ( = (61)),
which is derived within the phrase structure grammar. (115i) isderived from (116) by applying obligatory transformations only;hence, it is by definition a kernel sentence. (115ii) is formed from(116) by applying the obligatory transformations and T$_q$. Both(115iii) and (115iv) are formed by applying obligatory transformations,T$_q$, and T$_w$. They differ from one another only in the choice ofthe noun phrase to which T$_w$ applies. Suppose that we determinesentence types in general in terms of transformational history, i.e.,representation on the transformational level. Then the major subdivisionsof (115) are the kernel sentence (115i) on the one hand, and(115ii-iv), all of which have T$_q$ in their transformational representation,on the other. Thus (115ii-iv) are all interrogatives.(115iii-iv) form a special subclass of interrogatives, since they areformed by the additional subsidiary transformation T$_w$. Thus whenwe formulate the simplest transformational grammar for (115), wefind that the intuitively correct classification of sentences is given bythe resulting transformational representations.
* 9 Syntax and seamntics** 9.1 We have now found cases of sentences that are understood inmore than one way and are ambiguously represented on the transformationallevel (though not on other levels) and cases of sentencesthat are understood in a similar manner and are similarly representedon the transformational level alone. This gives an independentjustification and motivation for description of language in terms oftransformational structure, and for the establishment of transformationalrepresentation as a linguistic level with the same fundamentalcharacter as other levels. Furthermore it adds force to thesuggestion that the process of "understand i ng a sentence" can beexplained in part in terms of the notion of linguistic level. Inparticular, in order to understand a sentence it is necessary to knowthe kernel sentences from which it originates (more precisely, theterminal strings underlying these kernel sentences) and the phrasestructure of each of these elementary components, as well as thetransformational history of development of the given sentence fromthese kernel sentences.\cite{1} The general problem of analyzing theprocess of "understanding" is thus reduced, in a sense, to theproblem of explaining how kernel sentences are understood, thesebeing considered the basic 'content elements' from which the usual,more complex sentences of real life are formed by transformationaldevelopment.
\footnote{1} When transformational analysis is more carefully formulated, we find thatknowledge of the transformational representation of a sentence (which incorporatesthe phrase structure of the kernel strings from which the sentenceoriginates) is all that is necessary to determine the derived phrase structure ofthe transform.

In proposing that syntactic structure can provide a certain insightinto problems of meaning and understanding we have entered ontodangerous ground. There is no aspect of linguistic study moresubject to confusion and more in need of clear and careful formulationthan that which deals with the points of connection betweensyntax and semantics. The real question that should be asked is :"How are the syntactic devices available in a given language put towork in the actual use of this language?" I nstead of being concernedwith this very important problem, however, the study of interconnectionsbetween syntax and semantics has largely been dominatedby a side issue and a misformulated question . The issue hasbeen whether or not semantic information is required for discoveringor selecting a grammar; and the challenge usually posed bythose who take the affirmative in this dispute is : "How can youconstruct a grammar with no appeal to meaning?"
The remarks in \S 8 about possible semantic implications ofsyntactic study should not be misinterpreted as indicating supportfor the notion that grammar is based on meaning. In fact, thetheory outlined in \S\S 3-7 was completely formal and non-semantic.In \S 8, we have indicated briefly some ways in which the actual useof available syntactic devices can be studied. Perhaps this problemcan be elucidated somewhat further by a purely negative discussionof the possibility offinding semantic foundations for syntactic theory
** 9.2*** 9.2.1 A great deal of effort has been expended in attempting toanswer the question : "How can you construct a grammar with noappeal to meaning?" The question itself, however, is wrongly put,since the implication that obviously one can construct a grammarwith appeal to meaning is total ly unsupported. One might withequal justification ask : "How can you construct a grammar with noknowledge of the hair color of speakers?" The question that shouldbe raised is : "How can you construct a grammar?" I am notacquainted with any detailed attempt to develop the theory ofgrammatical structure in partially semantic terms or any specificand rigorous proposal for the use of semantic information in constructingor evaluating grammars. It is undeniable that "intuitionabout linguistic form" is very useful to the investigator of linguisticform (i.e., grammar). It is also quite clear that the major goal ofgrammatical theory is to replace this obscure reliance on intuitionby some rigorous and objective approach. There is, however, littleevidence that "intuition about meaning" is at all useful in theactual investigation of linguistic form. I believe that the inadequacyof suggestions about the use of meaning in grammatical analysisfails to be apparent only because of their vagueness and because ofan unfortunate tendency to confuse "intuition about linguisticform" with "intuition about meaning,"two terms that have incommon only their vagueness and their undesirability in linguistictheory. However, because of the widespread acceptance of suchsuggestion, it may be worthwhile to investigate some of thembriefly, even though the burden of proof in this case rests completelyon the linguist who claims to have been able to develop some grarrimaticalnotion in semantic terms.
*** 9.2.2 Among the more common assertions put forth as supportingthe dependence of grammar on meaning we have the following :
(117) (i) two utterances are phonemically distinct if and only ifthey differ in meaning ;
(ii) morphemes are the smallest elements that have meaning ;
(iii) grammatical sentences are those that have semanticsignificance ;
(iv) the grammatical relation subject-verb (i.e., $NP - VP$ asan analysis of Sentence) corresponds to the general'structural meaning' actor-action ;
(v) the grammatical relation verb-object (i.e., $Verb - NP$ asan analysis of $VP$) corresponds to the structural meaningaction-goal or action-object of action ;
(vi) an active sentence and the corresponding passive aresynonymous.
*** 9.2.3 A great many linguists have expressed the OpInIOn thatphonemic distinctness must be defined in terms of differentialmeaning (synonymity, to use a more familiar term), as proposed i n(117i). However, it is immediately evident that (117i) cannot beaccepted, as it stands, as a definition of phonemic distinctness.\cite{2} Ifwe are not to beg thc question, the utterances in question must betokens, not types. But there are utterance tokens that are phonemicallydistinct and identical in meaning (synonyms) and there areutterance tokens that are phonemically identical and different inmeaning (homonyms). Hence (117i) is false i n both directions.From left to right it is falsified by such pairs as "bachelor" and"unmarried man," or, even more seriously, by such absolutesynonyms as /eklmimiks/ and /iyklmimiks/ ("economics"), "adult"and "adUlt," /rresm/ and /reysm/, ("ration"), and many others,which may coexist even within one style of speech. From right toleft, (117i) is falsified by such pairs as "bank" (of a river) and "bank"(for savings),\cite{3} " metal" and "medal" (in many dialects), andnumerous other examples. In other words, if we assign two utterancetokens to the same utterance type on the basis of (117i), wewill simply get the wrong classification in a large number of cases.
\footnote{2}See my "Semantic considerations in grammar," Monograph no. 8, pp.141-53 ( 1 955), for a more detailed investigation of (1 1 7 i).
\footnote{3}Note that we cannot argue that "bank" in "the river bank" and " bank"in "the savings bank" are two occurrences of the same word, since this isprecisely the question under investigation. To say that two utterance tokensare occurrences of the same word is to say that they are not phonemicallydistinct, and presumably this is what the synonymity criterion (117i) is supposedto determine for us.
A weaker claim than (117i) might be advanced as follows.Suppose that we have an absolute phonetic system given in advanceof the analysis of any language, and guaranteed to be detai ledenough so that every two phonemically distinct utterances in anylanguage will be differently transcribed. It may now be the case thatcertain different tokens will be identically transcribed in thisphonetic transcription. Suppose that we define the "ambiguousmeaning" of an utterance token as the set of meanings of all tokenstranscribed identically with this utterance token. We might nowrevise (117i), replacing "meaning" by "ambiguous meaning." Thismight provide an approach to the homonymity problem, if we hadan immense corpus in which we could be fairly sure that each of thephonetically distinct forms of a given word occurred with each ofthe meanings that this word might have. It may be possible toelaboratei this approach even further to cope with the problem cfsynonyms. In such a way one might hope to determine phonemodistinctness by laborious investigation of the meanings of phoneticallytranscribed items in a vast corpus. The difficulty of determiningin any precise and realistic manner how many meanings severalitems may have in common, however, as well as the vastness of theundertaking, make the prospect for any such approach appearrather dubious.
*** 9.2.4 Fortunately, we do not have to pursue any such far-fetchedand elaborate program in order to determine phonemic distinctness.In practice, every linguist uses much more simple and straightforwardnon-semantic devices. Suppose that a linguist is interestedin determining whether or not "metal" and "medal" are phonemicallydistinct in some dialect of English. He will not Investigate themeanings of these words, since this information is clearly irrelevantto his purpose. He knows that the meanings are different (or he issimply not concerned with the question) and he is interested indetermining whether or not the words are phonemically distinct.A careful field worker would probablyuse the pair test,4 either withtwo informants or with an informant and a tape recorder. Forexample, he might make a random sequence of copies of theutterance tokens that interest him, and then determine whether ornot the speaker can consistently identify them. If there is consistentidentification, the linguists may apply an even stricter test, asking thespeaker to repeat each word several times, and running the pair testover again on the repetitions. If consistent distinguishability ismaintained under repetition, he will say that the words "metal" and"medal" are phonemically distinct. The pair test with its variantsand elaborations provides us with a clear operational criterion forphonemic distinctness in completely non-semantic terms.\cite{5}
\footnote{4}Cf. my "Semantic considerations of grammar," Monograph no. 8, pp.141-54 (1955) ; M. Halle, "The strategy of phonemics," Linguistics Today, Word10.197-209 (1954) ; Z. S. Harris, Methods in structural linguistics (Chicago,1951), pp.32f. ; C. F. Hockett, A manual of phonology = Memoir II, IndianaUniversity Publications in Anthropology and Linguistics (Baltimore, 1 955), p.146.
\footnote{5}Lounsbury argues in his "A semantic analysis of the Pawnee kinship usage,"Language 32. 1 58-94 (1 956), p. 1 90, that appeal to synonymity is necessary todistinguish between free variation and contrast : "If a linguist who knows noEnglish records from my lips the word cat first with a final aspirated stop andlater with a final preglottalized unreleased stop, the phonetic data will not tellhim whether these forms contrast or not. It is only when he asks me, hisinformant, whether the meaning of the first form is diff erent from that of thesecond, and I say it is not, that he will be able to proceed with his phonemicanalysis." As a general method, this approach is untenable. Suppose that thelinguist records /ekmamiks/ and /iykmamiks/, /viksm/ and /fiymeyl # faks/, etc.,and asks whether or not they are diff erent in meaning. He will learn that theyare not, and will incorrectly assign them the same phonemic analysis, if he takesthis position literally. On the other hand, there are many speakers who do notdistinguish "metal" from "medal," though if asked, they may be quite sure thatthey do. The responses of such informants to Lounsbury's direct question aboutmeaning would no doubt simply becloud the issue.
We can make Lounsbury's position more acceptable by replacing the question"do they have the same meaning?" with "are they the same word?" This willavoid the pitf alls of the essentially irrelevant semantic question, but it is hardlyacceptable in this form, since it amounts to asking the informant to do thelinguist's work ; it replaces an operational test of behavior (such as the pair test)by an informant's judgment about his behavior. The operational tests forlinguistic notions may require the informant to respond, but not to express hisopinion about his behavior, his judgment about synonymy, about phonemicdistinctness, etc. The informant's opinions may be based on all sorts of irrelevantfactors. This is an important distinction that must be carefully observed ifthe operational basis for grammar is not be trivialized.
It is customary to view non-semantic approaches to grammar aspossible alternatives to semantic approaches, and to criticize themas too complex, even if possible in principle. We have found, however,that in the case of phonemic distinctness, at least, exactly theopposite is true. There is a fairly straightforward and operationalapproach to the determination of phonemic distinctness in terms ofsuch non-semantic devices as the pair test. It may be possible inprinciple to develop some semantically oriented equivalent to thepair test and its elaborations, but it appears that any such procedurewill be quite complex, requiring exhaustive analysis of an immensecorpus, and involving the linguists in the rather hopeless attempt todetermine how many meanings a given phone sequence might have.
*** 9.2.5 There is one further difficulty of principle that should bementioned in the discussion of any semantic approach to phonemicdistinctness. We have not asked whether the meanings assigned todistinct (but phonemically identical) tokens are identical, or merelyvery similar. If the latter, then all of the difficulties of determiningphonemic distinctness are paralleled (and magnified, because of theinherent obscurity of the subject matter) in determining sameness ofmeaning. We will have to determine when two distinct meaningsare suff.ciently similar to be considered 'the same.' If, on the otherhand, we try to maintain the position that the meanings assignedare always identical, that the meaning of a word is a fixed andunchanging component of each occurrence, then a charge ofcircularity seems warranted. It seems.that the only way to upholdsuch a position would be to conceive of the meaning of a token as"the way in which tokens of this type are (or can be) used," the classof situations in which they can be used, the type of response thatthey normally evoke, or something of this sort. But it is difficult tomake any sense at all out of such a conception of meani ng without aprior notion of utterance type. It would appear, then, that evenapart from our earlier objections, any approach to phonemicdistinctness in semantic terms is either circular or is based on adistinction that is considerably more difficult to establish than thedistinction it is supposed to clarify.
*** 9.2.6 How, then, can we account for the widespread acceptance ofsome such formulation as (117i)? I think that there are two explanationsfor this. In part, i t is a consequence of the assumptionthat semantic approaches are somehow immediately given and aretoo simple to require analysis. Any attempt to provide a carefuldescription, however, quickly dispels this illusion. A semanticapproach to some grammatical notion requires as careful anddetailed a development as is justly required of any non-semanticapproach. And as we have seen, a semantic approach to phonemicdistinctness is beset by quite considerable difficulties.
A second source for such formulations as (117i), I believe, is aconfusion of "meaning" with "informant's response." We thusfind such comments on linguistic method as the following : "Inlinguistic analysis we define contrast among forms operationaIJy interms of difference i n meaning responses."\cite{6} We have observed in\S 9.2.3 that if we were to determine contrast by 'meaning response' inany direct way we would simply make the wrong decision in a greatmany places ; and if we try to avoid the difficulties that immediatelyarise, we are led to a construction that is so elaborate and has suchintolerable assumptions that it can be hardly taken as a seriousproposal. And we saw in \S 9.2.5 that there are apparently evnmore fundamental difficulties of principle. Hence, if we interpretthe quoted assertation literally, we must reject it as incorrect.
\footnote{6}F. Lounsbury, "A semantic analysis of the Pawnee kinship usage", Language32.158-94 (1956), p.191.
If we drop the word "meaning" from this statement, however, wehave a perfectly acceptable reference to such devices as the pairtest. But there is no warrant for interpreting the responses studiedin the pair test as semantic in any way.\cite{7} One might very welldevelop an operational test for rhyme that would show that "bill"and "pill" are related in a way in which "bill" and "ball" are not.There would be nothing semantic in this test. Phonemic identity isessentially complete rhyme, and there is no more reason forpostulating some unobserved semantic reaction in the case of"bill" and "bill" than in the case of "bill" and "pill."
\footnote{7} One should not be confused by the fact that the subject in the pair testmay be asked to identify the utterance tokens by meaning. He might just as wellbe asked to identify them by arbitrarily chosen numbers, by signs of the zodiac,etc. We can no more use some particular formulation of the pair test as anargument for dependence of grammatical theory on meaning than as an argumentthat linguistics is based on arithmetic or astrology.
It is strange that those who have objected to basing linguisitctheory on such formulations as (117i) should have been accused ofdisregard for meaning. It appears to be the case, on the contrary,that those who propose some variant of (117i) must be interpreting"meaning" so broadly that any response to language is caIJed"meaning." But to accept this view is to denude the term "meaning"of any i nterest or significance. I think that anyone who wishesto save the phrase "study of meaning" as descriptive of an importantaspect of linguistic research must reject this identification of"meaning" with "response to language," and along with it, suchformulations as (117i).


*** 9.2.7 It is, of course, impossible to prove that semantic notionsare of no use in grammar, just as it is impo�sible to prove theirrelevance of any other given set of notions. Investigation of suchproposals, however, invariably seems to lead to the conclusion thatonly a purely formal basis can provide a firm and productivefoundation for the construction of grammatical theory. Detailedi nvestigation of each semantically oriented proposal would gobeyond the bounds of this study, and would be rather pointless, butwe can mention briefly some of the more obvious counterexamplesto such familiar suggestion as (117).
Such morphemes as "to" in "I want to go" or the dummy carrier"do" i n "did he come?" (cf. \S 7.1) can hardly be said to have ameaning i n any independent sense, and it seems reasonable toassume that an i ndependent notion of meaning, if clearly given,may assign meaning of some sort to such non-morphemes as gl- in"gleam," "glimmer," "glow."\cite{8} Thus we have counterexamples tothe suggestion (117ii) that morphemes be defined as minimalmeaning-bearing elements. In § 2 we have given grounds forrejecting "semantic significance" as a general criterion for grammaticalness,as proposed in (l17iii). Such sentences as "John receiveda letter" or "the fighting stopped" show clearly the untenabilityof the assertion (117iv) that the grammatical relationsubject-verb has the 'structural meaning' actor-action, if meaning istaken seriously as a concept independent of grammar. Similarly,the assignment (117v) of any such structural meaning as action-goalto the verb-object relation as such is precluded by such sentences as"I will disregard his incompetence" or "I m issed the train . " Incontradiction to (117vi), we can describe circumstances in which a'quantificational' sentence such as "everyone in the room k nows atleast two languages" may be true, while the corresponding passive"at least two languages are known by everyone in the room" is false,under the normal interpretation of these sentences - e.g., if oneperson in the room knows only French and German , and anotheronly Spanish and Italian. This indicates that not even the weakestsemantic relation (factual equivalence) holds in general betweenactive and passive.
\footnote{8} See L. Bloomfield, Language (New York, 1 933), p.156; Z. S. Harris,Methods in structural linguistics (Chicago, 1951), p.177; O. Jespersen, Language(New York, 1922), chapter XX, for many further examples.

** 9.3 These counterexamples should not, however, blind us to thefact that there are striking correspondences between the structuresand elements that are discovered in formal, grammatical analysisand specific semantic functions. None of the assertions of (117)is wholly false ; some are very nearly true. It seems clear, then, thatundeniable, though only imperfect correspondences hold betweenformal and semantic features in language. The fact that the correspondencesare so inexact suggests that meaning will be relativelyuseless as a basis for grammatical description.\cite{9} Careful analysis ofeach proposal for reliance on meaning confirms this, and shows, infact, that important insights and generalizations about linguisticstructure may be missed if vague semantic clues are followed tooclosely. For example, we have seen that the active-passive relationis just one instance of a very general and fundamental aspect offormal linguistic structure. The similarity between active-passive,negation, declarative-interrogative, and other transformationalrelations would not have come to light if the active-passive relationhad been investigated exclusively in terms of such notions assynonymity.
\footnote{9} Another reason for suspecting that grammar cannot be effectively developedon a semantic basis was brought out in the particular case of phonemicdistinctness in § 9.2.5. More generally, it seems that the study of meaning isfraught with so many difficulties even after the linguistic meaningbearingelements and their relations are specified, that any attempt to study meaningindependently of such specification is out of the quest ion. To put it differently,given the instrument language and its formal devices, we can and should investigatetheir semantic function (as, e.g. , in R. Jakobson, "Beitrag wr allgemeinenKasuslehre," Travaux du Cercle Linguistique de Prague 6.240-88(1936)); but we cannot, apparently, find semantic absolutes, known in advanceof grammar, that can be used to determine the objects of grammar in any way.
The fact that correspondences between formal and semanticfeatures exist, however, cannot be ignored. These correspondencesshould be studied in some more general theory of language that willincl ude a theory of linguistic form and a theory of the use oflanguage as subparts. In § 8 we found that there are, apparently,fairly general types of relations between these two domains thatdeserve more intensive study. Having determined the syntacticstructure of the language, we can study the way in which thissyntactic structure is put to use in the actual functioning of language.An investigation of the semantic function of level structure,as suggested briefly in § 8, might be a reasonable step towards atheory of the interconnections between syntax and semantics. Infact, we pointed out in § 8 that the correlations between the formand use of language can even provide certain rough criteria ofadequacy for a linguistic theory and the grammars to which it leads.We can j udge formal theories i n terms of their ability to explain andclarify a variety of facts about the way in which sentences are usedand understood. In other words, we should like the syntacticframework of the language that is isolated and exh ibited by thegrammar to be able to support semantic description, and we shallnaturally rate more highly a theory of formal structure that leadsto grammars that meet this requirement more fully.
Phrase structure and transformational structure appear to providethe major syntactic devices available in language for organizationand expression of content. The grammar of a given language mustshow how these abstract structures are actually realized in the caseof the language in question, while linguistic theory must seek toclarify these foundations for grammar and the methods forevaluating and choosing between proposed grammars.
It is important to recognize that by introducing such considerationsas those of \S 8 into the meta theory that deals with grammarand semantics and their points of connection, we have not alteredthe purely formal character of the theory of grammatical structureitself. In \S\S 3-7 we outlined the development of some fundamentallinguistic concepts in purely formal terms. We considered theproblem of syntactic research to be that of constructing a devicefor producing a given set of grammatical sent.ences and of studyingthe properties of grammars that do this effectively. Such semanticnotions as reference, significance, and synonymity played no role inthe discussion . The outlined theory, of course, had serious gaps init - in particular, the assumption that the set of grammaticalsentences is given in advance is clearly too strong. and the notion of"simplicity" to which appeal was made explicitly or tacitly was leftunanalyzed. However, neither these nor other gaps in this developmentof grammatical theory can be filled in or narrowed, to myknowledge, by constructing this theory on a partially semanticbasis.
In \S\S 3-7, then, we were studying language as an instrument or atool, attempting to describe its structure with no explicit referenceto the way in which this instrument is put to use. The motivationfor this self-imposed formality requirement for grammars is quitesimple - there seems to be no other basis that will yield a rigorous.effective, and 'revealing' theory of linguistic structure. The requirementthat this theory shall be a completely formal discipline isperfectly compatible with the desire to formulate it in such a way asto have suggestive and significant interconnections with a parallelsemantic theory. What we have pointed out in \S 8 is that this formalstudy of the structure of language as an instrument may be expectedto provide insight into the actual use e>f language, i.e., into theprocess of understanding sentences.
** 9.4 To understand a sentence we must know much more than theanalysis of this sentence on each linguistic level. We must alsoknow the reference and meaning\cite{l0} of the morphemes or words ofwhich it is composed ; naturally, grammar cannot be expected to beof much help here. These notions form the subject matter forsemantics. I n describing the meaning of a word it is often expedient,or necessary, to refer to the syntactic framework in which thisword is usually emhedded ; e.g. , in describing the meaning of "hit"we would no doubt describe the agent and object of the action interms of the notions "subject" and "object", which are apparentlybest analyzed as purely formal notions belonging to the theory ofgrammar.\cite{11} We shall naturally find that a great many words ormorphemes of a single gram matical category are descri bed semanticallyin partially similar terms, e.g. verbs in terms 0(" subject andobject, etc. This is not surprising ; it means that the syntacticdevices available in the language are being used fairly systematically.We have seen, however, that so generalize from this fairly systematicuse and to assign 'structural meanings' to grammaticalcategories or constructions just as 'lexical meanings' are assigned towords or morphemes, is a step of very questionable validity.
\footnote{10} Goodman has argued-to my mind, quite convincingly-that the notionof meaning of words can at least in part be reduced to that of ref erence ofexpressions containing these words. See N. Goodman, "On likeness of meaning,"Analysis, vol.10, no.I (1949) ; idem, "On some diff erences about meaning,"Anal ysis, vol.13, no.4 (1953). Goodman's approach amounts to reformulatinga part of the theory of meaning in the much clearer terms of the theory ofreference, just as much of our discussion can be understood as suggesting areformulation of parts of the theory of meaning that deal with so-called "structuralmeaning" in terms of the completely nonsemantic theory of grammaticalstructure. Part of the difficulty with the theory of meaning is that "meaning"tends to be used as a catch-all term to include every aspect of language that weknow very little about. Insofar as this is correct, we can expect various aspectsof this theory to be claimed by other approaches to language in the course oftheir development.
\footnote{11}Such a description of the meaning of "hit" would then account automaticallyfor the use of "hit" in such transforms as "Bill was hit by John," "hittingBill was wrong," etc., if we can show in sufficient detail and generality thattransforms are 'understood' in terms of the underlying kernel sentences.


Another common but dubious use of the notion 'structuralmean ing' is with reference to the meanings of so-called 'grammaticallyfunctioning' morphemes such as ing, I)" prepositions, etc. Thecontention that the meanings of these morphemes are funda mentallydifferent from the meanings of nouns, verbs, adjectives, andperhaps other large classes, is often supported by appeal to the factthat these morphemes can be distributed in a sequence of blanks ornonsense syllables so as to give the whole the appearance of asentence, and in fact, so as to determine the grammatical category ofthe nonsense elements. For example, i n the sequence "Pirotskarulize etalically" we know that the three words are noun, verb,and adverb by virtue of the s, ize, and Iy, respectively. But thisproperty does not sharply distinguish 'grammatical' morphemesfrom others, since in such sequences as "the Pirots karul - yesterday"or "give him - water" the blanks are also determined as avariant of past tense, in the first case, and as "the", "some," etc., butnot "a," in the second. The fact that in these cases we were forcedto give blanks rather than nonsense words is explained by theproductivity or 'open-endedness' of the categories Noun, Verb,Adjective, etc., as opposed to the categories Article, Verbal Affix,etc. In general, when we distribute a sequence of morphemes in asequence of blanks we limit the choice of elements that can beplaced in the unfilled positions to form a grammatical sentence.Whatever differences there are among morphemes with respect tothis property are apparently better explained in terms of suchgrammatical notions as productivity, freedom of combination, andsize of substitution class than in terms of any presumed feature ofmeaning.
* 10 SUMMARYIn this discussion we have stressed the following points : The mostthat can reasonably be expected of linguistic theory is that itshall provide an evaluation procedure for grammars. The theoryof linguistic structure must be distinguished clearly from a manualof helpful procedures for the d iscovery of grammars, althoughsuch a manual will no doubt draw upon the results of linguistictheory, and the attempt to develop such a manual will probably(as it has in the past) contribute substantially to the formationof linguistic theory. If this viewpoint is adopted, there is littlemotivation for the objection to mixing levels, for the conceptionof higher-level elements as being literally constructed out oflower-level elements, or for the feeling that syntactic work ispremature until all problems of phonemics or morphology aresolved.
Grammar is best formulated as a self-contained study independentof semantics. In particular, the notion of grammaticalness can-110t be identified with meaningfulness (nor does it have any specialrelation, even approximate, to the notion of statistical order ofapproximation). In carrying out this independent and formal study,we find that a simple mode! of language as a finite state Markovprocess that produces sentences from left to right is not acceptable,and that such fairly abstract l inguistic levels as phrase structure andtransformational structure are required for the description ofnatural languages.
We can greatly simplify the description of English and gain newand important insight into its formal structure if we limit the directdescription in terms of phrase structure to a kernel of basic sen-tences (simple, declarative, active, with no complex verb or nounphrases), deriving all other sentences from these (more properly,from the strings that underlie them) by transformation, possiblyrepeated. Conversely, having found a set of transformations thatcarry grammatical sentences into grammatical sentences, we candetermine the constituent structure of particular sentences byinvestigating their behavior under these transformations with alternativeconstituent analyses.
We consequently view grammars as having a tripartite structure.A grammar has a sequence of rules from which phrase structure canbe reconstructed and a sequence of morphophonemic rules thatconvert strings of morphemes into strings of phonemes. Connectingthese sequences, there is a sequence of transformational rulesthat carry strings with phrase structure into new strings to whichthe morphophonemic rules can apply. The phrase structure andmorphophonemic rules are elementary in a sense in which thetransformational rules are not. To apply a transformation to astring, we must know some of the history of derivation of this string ;but to apply non-transformational rules, it is sufficient to know theshape of the string to which the rule applies.
As an automatic consequence of the attempt to construct thesimplest grammar for English in terms of the abstract levels developedin linguistic theory we find that the apparently irregularbehavior of certain words (e.g., "have," "be," "seem") is really acase of higher level regularity. We also find that many sentencesare assigned dual representations on some level, and many pairs ofsentences are assigned similar or identical representations on somelevel. In a significant number of cases, dual representation (constructionalhomonymity) corresponds to ambiguity of the representedsentence and similar or identical representation appearsin cases of intuitive similarity of utterances.
More generally, it appears that the notion of "understanding asentence" must be partially analyzed in grammatical terms. Tounderstand a sentence it is necessary (though not, of course,sufficient) to reconstruct its representation on each level, includingthe transformational level where the kernel sentences underlying agiven sentence can be thought of, in a sense, as the 'elementarycontent elements' out of which this sentence is constructed. In otherwords, one result of the formal study of grammatical structure isthat a syntactic framework is brought to light which can supportsemantic analysis. Description of meaning can profitably refer tothis underlying syntactic framework, although systematic semanticconsiderations are apparently not helpful in determining it in thefirst place. The notion of "structual meaning" as opposed to"lexical meaning", however, appears to be quite suspect, and it isquestionable that the grammatical devices available in language areused consistently enough so that meaning can be assigned to themdirectly. Nevertheless, we do find many important correlations,quite naturally, between syntactic structure and meaning; or, to putit differently, we find that the grammatical devices are used quitesystematically. These correlations could form part of the subjectmatter for a more general theory of language concerned withsyntax and semantics and their points of connection.